{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060ef9c0-f9e5-4833-a1da-30d0e7d90251",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. Preliminars:\n",
    "    1. Introduction to optimization\n",
    "    2. Mathematical notation and background\n",
    "2. Analysis\n",
    "    1. Unconstrained and constrained optimization with equalities. Optimality conditions.\n",
    "    2. Constrained optimization. Optimality conditions.\n",
    "    3. Convex sets and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7b70d-e2f9-4c4b-845a-5b47f2f63389",
   "metadata": {},
   "source": [
    "# 1. Preliminars\n",
    "\n",
    "## A. Introduction to optimization\n",
    "\n",
    "Optimization is an important issue in decision theory and analysis of physical systems. The natural ingredients are the objective (function) and the (potential) constrains. We make use of variables. The abstract process of identifying everything (objective, constrains, variables, etc) is known as modelling. That is, how do you go from real world to a problem one can solve. And how you conclude that this solution is real (solve the original problem).\n",
    "\n",
    "## B. Mathematical formulation\n",
    "\n",
    "1. The variable(s) or unkown(s), usually denoted by $x \\in \\mathbb{R}^n$.\n",
    "2. The objective (function), usually denoted by f. The scalar function f of the variable x is the one we want to maximize/minimize (optimize).\n",
    "3. The constrains (functions) are also scalar functions of the variable x defined in terms of equalities and/or inequalities: \n",
    "$$ g_j(x) \\leq 0, \\quad j = 1, \\dots, n  $$\n",
    "$$ h_i(x) = 0 , \\quad i = n+1, \\dots, m$$\n",
    "So the optimization problem considered is \n",
    "$$ min_{x \\in \\mathbb{R}^n} f (x) \\quad subject \\ to \\quad  g_j(x)\\leq 0, h_i(x) = 0$$\n",
    "\n",
    "**Definition:** The level set of f (of level $c \\in \\mathbb{R}$) is given by $L_c= \\{x \\in D: f(x)=c\\}\\subset \\mathbb{R}^n$, where $D$ is the domain of $f$. \n",
    "\n",
    "**Theorem (Bolzano)**: Let $f : [a, b] \\rightarrow \\mathbb{R}$ be a continuous function. Assume that $f (a)f (b) < 0$. Then there exists $c \\in (a, b)$ such that $f (c) = 0$.\n",
    "\n",
    "**Theorem (Weierstrass)**: Let $f: K \\subset \\mathbb{R} \\rightarrow \\mathbb{R}$, be continuous function such that $K$ is compact (closed and bounded), then $f$ is bounded (there exist $M$ such that $|f (x)| < M$ for all $x \\in K$ ) and f attaints its maximum and minimum values on $K$.\n",
    "\n",
    "**Definition (gradient vector)**: Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a real valued function that depends on $n$ variables. Let $a \\in \\mathbb{R}^n$ be an interior point of the domain of $f$, $D(f)$. Then the vector formed by the partial derivatives of f at the point a (assuming it exists) is known as the gradient vector\n",
    "$$ \\nabla f (a) = \\Bigg( \\frac{\\partial f}{\\partial x_1} (a) , \\dots, \\frac{\\partial f}{\\partial x_n} (a) \\Bigg) $$\n",
    "The partial derivatives of f measure the variation in f in the axis directions. But in many occasions we need to measure the variation of f in any direction, represented by a vector v ∈ Rn. One can show that w.l.o.g. we may assume $||v|| = 1$ (unitary vector).\n",
    "\n",
    "**Definition**: The directional derivative of $f$ in the direction of $v$ at the point $a$ is defined by\n",
    "$$\n",
    "D_v f (a) = lim_{h \\to 0} \\frac{f(a+hv) - f(a)}{h}\n",
    "$$\n",
    "\n",
    "**Theorem**: Let $\\alpha : (a, b) \\rightarrow \\mathbb{R}^n$ be a differentiable curve,\n",
    "$\\alpha(t) = (\\alpha_1(t), \\dots , \\alpha_n(t))^T$. Let f be a real valued differentiable function, i.e. $lim_{\\|h\\| \\to 0} \\frac{| f(a+h) - f(a) - (\\nabla f(a)9^T h|}{\\| h \\|} =0$.\n",
    "Then\n",
    "$$\n",
    "f (\\alpha(t)) = f (\\alpha_1(t), \\dots , \\alpha_n(t)),$$\n",
    "and\n",
    "$$\n",
    "\\frac{d}{dt} f (\\alpha(t)) = \\frac{\\partial f}{\\partial x_1} (\\alpha(t))\\alpha_1'(t) + \\cdots + \\frac{\\partial f}{\\partial x_n} (\\alpha(t))\\alpha_n'(t)\n",
    "$$\n",
    "\n",
    "**Corollary**: In the above notation (assume $f$ is differetiable), we have:\n",
    "$$\n",
    "D_v f(x) = \\frac{d}{dt} \\Big|_{t=0} f(x+tv) = \\sum_{j=1}^n \\frac{\\partial f(x)}{\\partial x_j} v_j = \\langle (\\nabla f(x))^T, v \\rangle\n",
    "$$\n",
    "\n",
    "**Theorem (the gradient vector)** Let $f : D \\subset \\mathbb{R}^n \\to \\mathbb{R}$ be a differentiable function at $a \\in D$, and $u \\in \\mathbb{R}^n$ is an unitary vector. The following statements hold.\n",
    "1. $D_u f(a) = (\\nabla f(a))^T \\cdot u = \\|\\nabla f(x)\\| cos (\\theta)$, where $\\theta$ is the angle between $u$ and $\\nabla f(a)$.\n",
    "2. The gradient vector $∇f (a)$ gives the maximum direction variation of f at the point a.\n",
    "3. The gradient vector at the point $a \\in D$ is orthogonal to the level curve passing through a.\n",
    "\n",
    "**Lemma (the tangent plane to the graf(f)):** Let $f$ be a differentiable function and let $a \\in D \\subset \\mathbb{R}^n$. Denote $x_{n+1} = f (x) = f (x_1, \\dots , x_n)$. The equation of the tangent plane of $graf(f )$ at the point $a$ is given by\n",
    "$$\n",
    "< ∇(F (a)), (x − a) > = 0$$\n",
    "where\n",
    "$\n",
    "F(x,x_{n+1})=f(x)−x_{n+1}$. In other words\n",
    "$$\n",
    "x_{n+1} =\\sum_{j=1}^n \\frac{\\partial f}{\\partial x_j} (a)(x_j −a_j).\n",
    "$$\n",
    "> Tangent plane formula !! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a319e-d9d0-4ed1-8476-7e78f35d40aa",
   "metadata": {},
   "source": [
    "# 2. Analysis\n",
    "\n",
    "## A. Unconstrained and oncstrained optimization with equalities. Opimality conditions. \n",
    "\n",
    "> Theory paper written \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de68a1-b17b-44f0-affc-ed7a898efed5",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "1. Part 1: \n",
    "    - Theory:\n",
    "        - Tangent plane, from diap 40 -> Done\n",
    "        - Linear aproximation + Dif matrix (jacobian) + Hessian\n",
    "    - Theory exercises\n",
    "        - Linear programming: diap 6/7\n",
    "        - Example: diap 8 –> Done\n",
    "        - Example 2: diap 9 -> Done (interesting formulation)\n",
    "        - CLassical problems: diap 11 -> Interesting\n",
    "        - The first price auction problem: diap 17 -> INTERESTING! *Nash Equilibrium* + second price auction\n",
    "        - Example linear approximation: diap 43\n",
    "    - Delivery problems\n",
    "2. Part 2: \n",
    "    - A\n",
    "        - Theorem B proof!: diap 24-29 !!!!\n",
    "        - Exercises: \n",
    "            - Diap 32\n",
    "    - B: \n",
    "        - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02adc0ea-7692-476b-a213-96d3868f43ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print((2*97-98)/3)\n",
    "print(97-2*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77540c8e-81a6-4fe7-9b34-9e3a0d9f39af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
