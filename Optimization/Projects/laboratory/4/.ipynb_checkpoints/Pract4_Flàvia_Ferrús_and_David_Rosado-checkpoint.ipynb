{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3eMAkgs0Y7kS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp as e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvX7jhRHZ7fh"
   },
   "source": [
    "Flàvia Ferrús and David Rosado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWY34MBZaogK"
   },
   "source": [
    "## Proposed experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_mfmkCNbFeM"
   },
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UH2j9n4gba6s"
   },
   "source": [
    "Let us implement the Sequential Quadratic Optimization (SQO) method by applying $\\alpha^k=1$ and iteratively update the current point to obtain the next. Let us start by define the functions that we need. Remember that the Lagrangian is given by\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\textbf{x}, \\lambda) = f(\\textbf{x}) - \\lambda h(\\textbf{x}),\\hspace{0.5cm}\\textbf{x}\\in\\mathbb{R}^n.\n",
    "\\end{align*}\n",
    "In our case, $n=2$ and $f(x,y)=e^{3x} + e^{-4y}$ and $h(x,y)=x^2+y^2-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cNS0bm_fff8y"
   },
   "outputs": [],
   "source": [
    "#Definitions of the functions\n",
    "def f(x,y):\n",
    "  return e(3*x) + e(-4*y)\n",
    "def h(x,y):\n",
    "  return x**2 + y**2 -1\n",
    "def grad_f(x,y):\n",
    "  return np.array([3*e(3*x), -4*e(-4*y)])\n",
    "def grad_h(x,y):\n",
    "  return np.array([2*x, 2*y])\n",
    "def hessian_f(x,y):\n",
    "  H = np.zeros((2,2))\n",
    "  H[0,0] = 9*e(3*x)\n",
    "  H[1,0] = 0\n",
    "  H[0,1] = 0\n",
    "  H[1,1] = 16*e(-4*y)\n",
    "  return H\n",
    "def hessian_h(x,y):\n",
    "  H = np.zeros((2,2))\n",
    "  H[0,0] = 2\n",
    "  H[1,0] = 0\n",
    "  H[0,1] = 0\n",
    "  H[1,1] = 2\n",
    "  return H\n",
    "def lagran(x,y,lanbda):\n",
    "  return f(x,y) - lanbda*h(x,y)\n",
    "def lagran_gradx(x,y,lanbda):\n",
    "  return grad_f(x,y) - lanbda*grad_h(x,y)\n",
    "def lagran_hessianx(x,y,lanbda):\n",
    "  return hessian_f(x,y) - lanbda*hessian_h(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFTWDpbJhb6_"
   },
   "source": [
    "Let us implement the SQO algorithm with Newton's method to solve\n",
    "\\begin{cases}\n",
    "\\text{min}\\hspace{0.2cm}f(x,y)\\\\\n",
    "\\text{subject to}\\hspace{0.2cm} h(x,y)=0\n",
    "\\end{cases}\n",
    "where $f$ and $h$ are defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "buWklC9ibOn0"
   },
   "outputs": [],
   "source": [
    "#Function that implements the SQO with Newton's method\n",
    "def Newton_algorithm(x0,y0,lanbda_0,alpha,max_iter,tol):\n",
    "  for i in range(0,max_iter):\n",
    "    #Build the matrix A to solve Ax=b\n",
    "    A = np.zeros((3,3))\n",
    "    for k in range(0,2):\n",
    "      for j in range(0,2):\n",
    "        A[k,j] = lagran_hessianx(x0,y0,lanbda_0)[k,j]\n",
    "    for k in range(0,2):\n",
    "      A[2,k] = -grad_h(x0,y0)[k]\n",
    "      A[k,2] = -grad_h(x0,y0)[k]\n",
    "   #Build the vector b\n",
    "    b = np.zeros(3)\n",
    "    for k in range(0,2):\n",
    "      b[k] = -lagran_gradx(x0,y0,lanbda_0)[k]\n",
    "    b[2] = h(x0,y0)\n",
    "    #Solve the system using the python solve\n",
    "    delta = np.linalg.solve(A,b)\n",
    "    #Actualize the variables\n",
    "    x0 = x0 + alpha*delta[0]\n",
    "    y0 = y0 + alpha*delta[1]\n",
    "    lanbda_0 = lanbda_0 + alpha*delta[2]\n",
    "    if np.linalg.norm(lagran_gradx(x0,y0,lanbda_0))<tol:\n",
    "      print('Iterations:',i)\n",
    "      print('x = (x, y) =',x0,y0)\n",
    "      print('lamba=',lanbda_0)\n",
    "      return x0,y0,lanbda_0\n",
    "      break\n",
    "  return x0,y0,lanbda_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzQoGMRHtm_3",
    "outputId": "76d2cb21-19aa-4d01-ee39-0aeed31d0b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 2\n",
      "x = (x, y) = -0.7483381762503777 0.663323446868971\n",
      "lamba= -0.21232390186241443\n"
     ]
    }
   ],
   "source": [
    "x,y,lanbda = Newton_algorithm(-1,1,-1,1,100,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wltc90trl8b"
   },
   "source": [
    "We can observe in the *pdf* file that the solution of this problem is $(x^*,y^*)=( -0.74834,0.66332)$ and $\\lambda^*=−0.21233$. Notice that we reach the correct solution of the problem in two iterations choosing $\\epsilon = 10^{-3}$. Evidently, if we set a lower $ϵ$, the number of iterations will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaNLxR7BbI50"
   },
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTPqwbL0uoEr"
   },
   "source": [
    "Let us choose starting points that are farther away of the optimal solution to see if the algorithm works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZwt3OWnbPPK",
    "outputId": "6d461eb4-779d-4c27-81c3-89b20be5fd9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting points are (x0,y0,lanbda_0)= (1.1897569607285545, -0.18888208027615416, 0.8217764510629444)\n",
      "Iterations: 5\n",
      "x = (x, y) = 0.9949825879806212 -0.10004824333248946\n",
      "lamba= 29.827861700223693\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (1.700359514272526, -0.03637257311024056, 1.2836457959895375)\n",
      "Iterations: 9\n",
      "x = (x, y) = -0.7483594868892595 0.6633030894123038\n",
      "lamba= -0.21230688436242776\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (1.7738036897812932, -0.9915299122890213, 0.7993198082158964)\n",
      "Iterations: 5\n",
      "x = (x, y) = 0.910413052541143 -0.41370064616629076\n",
      "lamba= 25.293845609301567\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (2.328579083310692, -1.6307090402545716, 1.5751935995040847)\n",
      "Iterations: 7\n",
      "x = (x, y) = 0.9104132322159664 -0.4137000699470232\n",
      "lamba= 25.293855122901604\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (3.16346236271288, -1.5590286171787615, 1.2907586410378216)\n",
      "Iterations: 7\n",
      "x = (x, y) = 0.9949826916769633 -0.1000493219931445\n",
      "lamba= 29.827804650979605\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (3.3192036234516706, -2.5124279567672447, 2.3107696146935717)\n",
      "Iterations: 9\n",
      "x = (x, y) = 0.9104132307507797 -0.4137000746262301\n",
      "lamba= 25.293855046033784\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (4.283425175362312, -2.835234088433967, 2.3163031010541424)\n",
      "Iterations: 16\n",
      "x = (x, y) = 0.9104132286484214 -0.41370008132930164\n",
      "lamba= 25.29385493609318\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (4.621694657406245, -3.9570232313899525, 2.475425471876282)\n",
      "Iterations: 8\n",
      "x = (x, y) = 0.014345670339172173 -0.9998983626733192\n",
      "lamba= 109.16276021937368\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (4.960993815351757, -3.9972938172613754, 3.1562570753972192)\n",
      "Iterations: 10\n",
      "x = (x, y) = 0.01434522860487264 -0.99989710310674\n",
      "lamba= 109.16259750022532\n",
      "\n",
      "\n",
      "\n",
      "The starting points are (x0,y0,lanbda_0)= (5.924491700186166, -4.327951286438212, 3.2787863401935513)\n",
      "Iterations: 10\n",
      "x = (x, y) = 0.9104132301086224 -0.41370007665941055\n",
      "lamba= 25.293855012919575\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us create random points, farther away of the optimal solution and implement the algorithm\n",
    "for i in range(1,11):\n",
    "  x0 = float(np.random.rand(1) + i/2)\n",
    "  y0 = float(np.random.rand(1) - i/2)\n",
    "  lanbda_0 = float(np.random.rand(1) + i/4)\n",
    "  print('The starting points are (x0,y0,lanbda_0)=',(x0,y0,lanbda_0))\n",
    "  Newton_algorithm(x0,y0,lanbda_0,1,100,1e-3)\n",
    "  print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg0RTfhix9cx"
   },
   "source": [
    "Notie that in most cases, the method does not work. That is beacuse Newton algorithm only works in a local way, so if we choose starting points that are farther away of the optimal solution, the method may not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rucT-KbKbJBT"
   },
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFIN2Cymy7n_"
   },
   "source": [
    "Let us define the merit function $\\mathcal{M}$ and perform a classical gradient descent( with backtraking) algorithm, in order to deal with the problem of starting points that are farther away of the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUejGIF4bTG-"
   },
   "outputs": [],
   "source": [
    "#Definition of the merit function and its gradient\n",
    "def merit(x, y, rho=10):\n",
    "    return f(x, y) + rho * h(x, y)**2\n",
    "\n",
    "def grad_merit(x, y, rho=10):\n",
    "    return grad_f(x, y) + 2 * rho * h(x, y) * grad_h(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tM2GCKa40K7H"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with backtracking\n",
    "def gradient_descent(f,grad_f,w0,w1,tol):\n",
    "  x_0=np.zeros(2)\n",
    "  x_0[0]=w0\n",
    "  x_0[1]=w1\n",
    "  while True:\n",
    "      alpha=1\n",
    "      grad = grad_f(x_0[0],x_0[1])\n",
    "      x_k=x_0-alpha*grad/np.linalg.norm(grad)\n",
    "      while f(x_k[0],x_k[1])>=f(x_0[0],x_0[1]):\n",
    "        alpha=alpha/2\n",
    "        x_k=x_0-alpha*grad/np.linalg.norm(grad)\n",
    "      if abs(f(x_k[0],x_k[1]) - f(x_0[0],x_0[1])) < tol  or np.linalg.norm(grad/np.linalg.norm(grad)) < tol:\n",
    "        return x_k\n",
    "      else:\n",
    "        x_0=x_k\n",
    "        \n",
    "  return x_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTJWXJoc2HeO"
   },
   "source": [
    "Let us test this method with a point farther away of the optimal solution and oberve if the result is close to the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kT61M-E2pBS",
    "outputId": "2a928d83-247d-4dd2-960e-5c3a2a72160a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.967853701779216\n",
      "The solution of the gradient descent using the merit function is (x,y)= (-0.3236139698916473, 0.8825391209903108)\n",
      "10.973207959981872\n",
      "The solution of the gradient descent using the merit function is (x,y)= (-0.5179680757923558, 0.8241081695219867)\n"
     ]
    }
   ],
   "source": [
    "w0 = float(np.random.rand(1) + 34/2)\n",
    "w1 = float(np.random.rand(1) - 34/2)\n",
    "print(w0)\n",
    "res1 = gradient_descent(merit, grad_merit,w0,w1,1e-3)\n",
    "print('The solution of the gradient descent using the merit function is (x,y)=',(res1[0],res1[1]))\n",
    "w0 = float(np.random.rand(1) + 20/2)\n",
    "w1 = float(np.random.rand(1) - 20/2)\n",
    "print(w0)\n",
    "res2 = gradient_descent(merit, grad_merit,w0,w1,1e-3)\n",
    "print('The solution of the gradient descent using the merit function is (x,y)=',(res2[0],res2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE-xdn-b3m_Z"
   },
   "source": [
    "Notice that we are getting closer to the optimal solution!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaLwfVsbJJD"
   },
   "source": [
    "### Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHhrabut3_zg"
   },
   "source": [
    "As we have seen and is said in the $\\textit{pdf}$ file, the minimizers of the merit function do not necessarily have to coincide with the minimizers of the constrained problem. Therefore, we will build an algorithm that consists in the following: \n",
    "+ Start with the merit function to obtain an approximation to the optimal point we are looking for.\n",
    "+ Once an approximation to the solution is found, use the Newton-based method to find the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6xCvXsD5TPY"
   },
   "source": [
    "We will use the starting points used in the previous experiment. Notice that we have the first of the algorithm already implemented. The aproximation points obtained with the merit function are stored in $res1$ and $res2$. Let us apply now the Newton-based algorithm to find the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxeqSIqx5RbR",
    "outputId": "18a2c15c-a6a2-403b-ecab-e1a179335607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 2\n",
      "x = (x, y) = -0.7483353457092601 0.6633208782631613\n",
      "lamba= -0.21232389024350248\n",
      "Iterations: 2\n",
      "x = (x, y) = -0.7483106326338553 0.6633717041241959\n",
      "lamba= -0.2122554628381516\n"
     ]
    }
   ],
   "source": [
    "sol1 = Newton_algorithm(res1[0],res1[1], -1, 1, 100,1e-3)\n",
    "sol2 = Newton_algorithm(res2[0],res2[1], -1, 1, 100,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzkvP7CU8XCN"
   },
   "source": [
    "Finally, we obtain the expected result!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpeg7ql8ykL5"
   },
   "source": [
    "## Extra experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp1YevHT9I5j"
   },
   "source": [
    "We seek to apply the minimum possible force to move a particle with mass $m$ from an initial point $x_0 = (0,0,0)$, to the final point $x_1 = (1,0,0)$ in $T=1$ seconds, in absence of any other body forces. Let's consider the problem uni-dimensional, since we can consider the reference system to be centered at the initial point of the particle and assume the particle is moving in the $x$-axis direction. Thus, the generalized coordinate is given by $q=x$ and momentum $p= m \\dot{q}$. Since there are no field no conservative acting on the system and there are no no stationary constraints acting over the free particle, we have that the Hamiltonian corresponds to the total energy of the system, this is \n",
    "$$ \n",
    "H = p \\dot{q} - L = E_T = E_K + E_P\n",
    "$$\n",
    "where $E_K, E_P$ are the kinetic and potential energies of the system, $L=E_k - E_p$ is the Lagrangian. Thus, under these assumptions we have $E_K = \\frac{1}{2} m \\dot{q}^2 = \\frac{p^2}{2m}$, where clearly $\\dot{q} = \\frac{\\partial q}{\\partial t}$, and $E_P = - W_F= -\\int F(t) dr$ where the $W_F$ denotes the work experienced by the force $F(t)$ that we apply to the particle. Given that we assume that the force is conservative and thus its work does not depend on the path followed, we have $F(t)=f$, and therefore, the hamiltonian has the following expression:  \n",
    "$$\n",
    "H(p,q,t) = \\frac{1}{2} m \\dot{q}^2 - F q \n",
    "$$\n",
    "Thus, since $H=E_T$ and due to the principle of conservation of energy we have \n",
    "$$\n",
    "\\frac{\\partial H}{\\partial t} = 0 \\iff m \\dot{q} \\ddot{q} - f\\dot{q} = 0 \\iff f = m \\ddot{q}\n",
    "$$\n",
    "Note that we have recovered the second Newton's law, and we can therefore find $f$ by solving the differential equation obtained in terms of $f$ and then compute the corresponding value of $f$ by plugging in the initial conditions fixed:\n",
    "$$\n",
    "\\ddot{q} = \\frac{f}{m} \\iff \\dot{q} \\int_0^t\\frac{f}{m}ds = \\frac{ft}{m} \\iff q(t) = \\int_0^t\\frac{fs}{m}ds = \\frac{f t^2}{2m}\n",
    "$$\n",
    "Using now the initial conditions we have $f= 2m$. \n",
    "\n",
    "However, consider now the case in which we want the particle to reach point $x_1$ and stay there. Observe now that the force used $F(t)$ is not conservative this time, and we may consider the non stationary constraints over the Hamiltonian. We seek to find now the minimum force $F(t)=f$, i.e. $min |F(t)|$ constrained to the second Newton's law: $F(t) = m \\ddot{q} = \\dot{p} \\iff \\dot{q}= p/m, \\ \\dot{p}=f$. Thus, considering the Lagrange multipliers on this system we have \n",
    "$$\n",
    "L' = |f(t)|^2 + \\lambda_1 \\frac{p}{m} + \\lambda_2 f(t)\n",
    "$$\n",
    "Therefore, the Euler-Lagrange equations with the new generalized variable to be $p, f$ are given by the corresponding partial derivatives we have:\n",
    "$$\n",
    "\\frac{\\partial L'}{\\partial f } = \\frac{d}{dt} \\frac{\\partial L'}{\\partial \\dot{f} } = 0 \\iff 2 f + \\lambda_2 = 0 \\iff f = -\\lambda_2/2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similarly, we have\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L'}{\\partial q} = 0 =& \\frac{d}{dt}\\frac{\\partial L'}{\\partial \\dot{q}} = \\frac{d}{dt} \\lambda_1 = \\dot{\\lambda_1} \\\\\n",
    "\\frac{\\partial L'}{\\partial p} = & \\frac{\\lambda_1}{m} = \\frac{d}{dt}\\frac{\\partial L'}{\\partial \\dot{p}}  = \\dot{\\lambda_2}\n",
    "\\end{align*}\n",
    "And therefore $\\lambda_1 = const = a$ and $\\lambda_2 = \\frac{at+b}{m}$. Consequently we have that $f=- \\frac{at+b}{2m}$, and by solving the corresponding differential equations we have\n",
    "\\begin{align*}\n",
    "p(t) = \\int_0^t f(s) ds &= \\frac{at^2 }{4m} + \\frac{bt}{2m} \\\\\n",
    "q(t) = \\int_0^t p(s) ds &= \\frac{1}{m} \\Big[ \\frac{at^3 }{12m} + \\frac{bt^2}{4m} \\Big]\n",
    "\\end{align*}\n",
    "Finally, by fixing the boundary conditions of $p(1) = 0 $ and $q(1)= 1$ we get that $a = -24m^2$ and $b = -a/2 = 12m^2$, and consequently we get\n",
    "$$\n",
    "\\boxed{f(t) = \\frac{at + b}{m} = -24mt + 12m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
