{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1cafb5-4066-4ae2-ae0f-78e78ba74784",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. **Linear models**\n",
    "    1. **Ordinary least squares regression**: minimize$\\sum_i (y_i - h(x_i;a,b))^2$, with $h(x) = a^Tx + b$. \n",
    "    2. **Logistic regression:** *linear classifier** that is based on maximum likelihood principles to jointly model the classification boundary and the certainty of the fit. It is based on fitting a logistic function to the data*. $p(x_i|y) = \\frac{1}{1+e^{-(a^Tx+b)}}$\n",
    "    3. **Support Vector Machines (SVM)** is a prototypical example of discriminative learning. In this setting one explicitly assumes a function model class of the boundary. The classical model for SVM is a linear model. The critical subset of data points are called **Support Vectors**. If any of those points disappear the boundary changes.  The decision boundary depends on the support vectors, thus we have to store them in our model.\n",
    "        - Hard margin: separable case: min $\\|a\\|_2$ subject to $y_i(a^T x_i + b) \\geq 1$\n",
    "        - Soft margin: non-separable case, we introduce a part taking into account the amount of violation on the constraints: \n",
    "        $$ min \\|a\\|_2 + C \\sum_i \\xi_i \\ s.t. \\ y_i(a^T x_i + b) \\geq 1 - \\xi_i \\iff min \\|a\\|_2 + C \\sum_i max(0, 1- y_i(a^t x_i +b) $$\n",
    "2. **Kernels**:\n",
    "    1. **Reproducing kernel hilbert space (RKHS)**: A Hilbert space $\\mathcal{H}$ (RKHS) if the evaluation functionals are bounded If $\\mathcal{H}$ is a RKHS, then for each $x \\in X$ there exists, by the Riesz representation theorem a function $K_x$ of $\\mathcal{H}$ (called representer) with the reproducing property. **The notion of complexity now can be defined in terms of the norm of a function in the Hilbert space**Thus,\n",
    "$$\n",
    "f^*=\\underset{f\\in \\mathcal{H}}{\\operatorname{arg\\,min}}\\frac{1}{n}\\sum_i{\\mathcal{L}(f(x_i),y_i)}+\\lambda\\|f\\|_{\\mathcal{H}}^2\n",
    "$$\n",
    ">By the **representer's theorem** the optimal solution after solving the minimization of the former problem is given by:\n",
    "$$\n",
    "f^*(\\cdot)=\\sum_i{\\alpha_i K(x_i,\\cdot)}\n",
    "$$\n",
    "This is a remarkable fact because it converts the problem of looking for a generic function in an infinite-dimensional space into that of finding the optimal $\\alpha_i\\in \\mathbb{R}$ in a finite dimensional space.\n",
    "$$\n",
    "\\mathcal{F}_x[f]=f(x)=\\langle K_x, f\\rangle=\\langle K(x,\\cdot ), f(\\cdot) \\rangle\n",
    "$$\n",
    "\n",
    "3. **Ensemble models**:*Ensemble learning is based on the aggregation of decisions of different classifiers. For ensemble learning to work classifiers must be diverse.*\n",
    "4. **Neural network and deep learning models**\n",
    "5. **Unsupervised learning**:\n",
    "    1. **Manifold and representation learning**: *Non-linear dimensionality reduction*\n",
    "        - **Multidimensional Scaling (MDS)**: *construct a constellation of points in Euclidean space by using information about the distances between the samples*\n",
    "        - **Isomap**: *extension of MDS. Perform MDS in the **geodesic space** of the nonlinear data manifold*\n",
    "        - **Locally linear Embedding**: *linearly approximating the manifold around each sample and then finding the low dimensional representation that best reconstructs that sample according to the linear manifold* \n",
    "        - **Stochastic Neighbor Embedding (SNE)**: changes the notion of distance to that of **conditional probabilities**. The **similarity** of a data point $x_i$ to $x_j$ is modelled by the **conditional probability** $p_{i|j}$ that $x_i$ would select $x_j$ as its neighbors if neighbors were selected in proportion to their pdf under a Gaussian centered at $x_j$.*\n",
    "        - **T-Distributed Stochastic Neighbor Embedding**: *modifies this **symmetric SNE** by changing the Gaussian distribution in the projected space by a **t-distribution*** \n",
    "    2. **Autoencoders**: *they work by **compressing the data**, this is, we have a **bottlenck**, which is a good representation for reconduction purposes.* \n",
    "        - We have two ways of building an autoencoder:\n",
    "            - **Physical bottleneck (BN)**: *compression of the input data towards the middle reduced layer, which has the necessary information to decode all the data, the **encoder** part >, and then the decompression into the output data, the **decoder** part <. Structure: ><.* \n",
    "            - **Logical bottleneck (BN)**: *the inverse model, in which we have an expansion and then a compression <>. In this case we introduce **spartion** (most of the neurons in the middle are zero). Structure: <>.* \n",
    "        - We can introduce the notion of distribution in Autoecoders:\n",
    "            + **Variational Autoencoders:** *approximating the desired distribution $p(z|x)$ by a controlled set of base functions $q(z)$. Usually, we use mean field approximation in which we assume that all variables in $z$ are independent and Gaussian. This is $q_x(z) = \\prod q_x(z^{(i)}) = \\prod \\mathcal{N}(\\mu^{(i)}(x),\\sigma^{(i)}(x))$.*\n",
    "            + **Generative adversarial networks**: *trick for **guiding a network into mimicking a sampled data distribution**. In a nutshell it consists of a two part process that get repeated interativelly until we have a nearly perfect data generator, this is, until we get a decoder that produces fake data which cannot be distinguished from the real data.:*\n",
    "                - **Part 1: The generator** A network plays the role of a generator. We input data coming from one well-known distribution such as a Normal distribution and the output is expected to be the type of data we desire, e.g. cat images, words, etc. Just as we saw with the decoder of the Auto-encoder.\n",
    "                - **Part 2: The discriminator** In order for the generator to be able to achieve its goal here comes the clever trick. A discriminator or classifier is trained with data coming from the true source we want to mimick and with the data coming from the generator. These are mixed. And the role of the discriminator is to tell apart real from fake data. \n",
    "\n",
    "6. **Mixture models**: *clustering algorithms that are related to inferences of multiple gaussians. \n",
    "    - *Then the clustering techniques are based on grouping partition unlabeled examples (unsupervised data) into disjoint subsets of clusters, such that:*\n",
    "        + Examples within a cluster are similar (high intra-class similarity)\n",
    "        + Examples in different clusters are different (low inter-class similarity)\n",
    "    - We have different clustering techniques: \n",
    "        + **K-means**: *looks at the problem of identifying groups, or clusters of data points in a multidimensional space. Our goal is to partition the space in $K$ clusters. In k-means, the clusters are represented by $k$ prototypes $\\{m^{(k)}\\}$. We may think about these prototypes as the centers of the clusters. K-means then consists of compressing groups into prototypes. It is important that the number of prototypes is fixed a priori, we are stating the number of clusters before the algorithm starts.* The solution of this problem can be done by iterating two steps, in which we solve first for one while keeping the other fixed, and then we optimize the other based on the former solution, called **Expectation Maximization (EM)**, where the expectation step **E** and the maximization step **M** alternates. In this case we may choose an initial value of the prototypes $\\{m^{(k)}\\}$ and then alternate the following steps:\n",
    "            + **Step E:** We minimize the objective with respect to $r_i^{(k)}$ keeping the values of the prototypes fixed.\n",
    "            + **Step M:** We minimize the objective with respect to $\\{m^{(k)}\\}$ keeping the values of the responsibilities fixed.\n",
    "        + **Soft k-means**: *We set a **stiffness parameter** $\\beta$ and we change the assignment rule considering the similarity to each of the means. Observe that the **similarity is now defined with respect to all possible means**, this is each point resebles something to each mean. This will mean that all data contribute to the assignment.*\n",
    "        + **Mixture of Gaussians**: *we conduct the clustering by preassuming that the different prototypes have Gaussian distribution. Each distribution is found by conducting the maximum likelihood of the assumed Gaussian, to determine the corresponding parameters of the distribution.* \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34902fb4-a8a0-47e4-aadb-208db6279c94",
   "metadata": {},
   "source": [
    "# 10. Linear models \n",
    "\n",
    "## 10.1 Ordinary least squares regression\n",
    "\n",
    "We have seen linear models before several times. Let us recall what they are about. The underlying model is a linear one, this is\n",
    "\n",
    "$$h(x) = a^Tx + b$$\n",
    "\n",
    "In the context of regression, the use of this model can be associated to the minimization of the squared mean error loss, this is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{a,b}{\\text{minimize}} &\\quad \\sum_i (y_i - h(x_i;a,b))^2\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "As we saw in other notebooks, this problem can be easily solved using iterative approaches or even using a closed form solution.\n",
    "\n",
    "## 10.2 Logistic regression\n",
    "\n",
    "Logistic regression is a **linear classifier** that is based on maximum likelihood principles to jointly model the classification boundary and the certainty of the fit. It is based on fitting a logistic function to the data.\n",
    "\n",
    "$$p(x_i|y) = \\frac{1}{1+e^{-(a^Tx+b)}}$$\n",
    "\n",
    "## 10.3 Supor vector machine\n",
    "\n",
    " The Support Vector Machine classifer finds the boundary with maximum distance/**margin** to both classes.\n",
    "- It implicitly models the notion of noise. One expects that the boundary with maximum margin will be robust to small perturbations in the data.\n",
    "- A maximum margin classifier has a unique solution in the separable case.\n",
    "\n",
    "Observe that there is a critical subset of data points. These are called **Support Vectors**. If any of those points disappear the boundary changes.  The decision boundary depends on the support vectors, thus we have to store them in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf563c-22bf-444c-bdf5-02027c6fb9db",
   "metadata": {},
   "source": [
    "# 11. Kernels\n",
    "\n",
    "**Definition:** A Hilbert space $\\mathcal{H}$ is a ***reproducing kernel Hilbert space***\n",
    "(RKHS) if the evaluation functionals are bounded, i.e. if there exists a M such that\n",
    "$$|\\mathcal{F}_x[f]|=|f(x)|\\leq M \\|f\\|_{\\mathcal{H}} \\forall f \\in \\mathcal{H}\n",
    "$$\n",
    "**Reproducing Kernel Hilbert Space\n",
    "Property**\n",
    "If $\\mathcal{H}$ is a RKHS, then for each $x \\in X$ there exists, by the\n",
    "Riesz representation theorem a function $K_x$ of $\\mathcal{H}$ (called\n",
    "representer) with the reproducing property\n",
    "$$\n",
    "\\mathcal{F}_x[f]=f(x)=\\langle K_x, f\\rangle=\\langle K(x,\\cdot ), f(\\cdot) \\rangle\n",
    "$$\n",
    "\n",
    "Observe that the evaluation of a funtion on a given point corresponds to the inner product between the function and the Reproducing Kernel function evaluated at that point.\n",
    "\n",
    "**The notion of complexity now can be defined in terms of the norm of a function in the Hilbert space**\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "f^*=\\underset{f\\in \\mathcal{H}}{\\operatorname{arg\\,min}}\\frac{1}{n}\\sum_i{\\mathcal{L}(f(x_i),y_i)}+\\lambda\\|f\\|_{\\mathcal{H}}^2\n",
    "$$\n",
    ">By the **representer's theorem** the optimal solution after solving the minimization of the former problem is given by:\n",
    "$$\n",
    "f^*(\\cdot)=\\sum_i{\\alpha_i K(x_i,\\cdot)}\n",
    "$$\n",
    "This is a remarkable fact because it converts the problem of looking for a generic function in an infinite-dimensional space into that of finding the optimal $\\alpha_i\\in \\mathbb{R}$ in a finite dimensional space.\n",
    "\n",
    "> Let us write down the value of $\\|f^*\\|^2_\\mathcal{H}$,\n",
    "$$\\|f^*\\|^2_\\mathcal{H}=\\langle f^*,f^* \\rangle_\\mathcal{H}=\\langle \\sum_j{\\alpha_j K(x_j,x)},\\sum_l{\\alpha_l K(x_l,x)} \\rangle_\\mathcal{H}\n",
    "$$ by linearity\n",
    "$$\n",
    " \\sum_j \\sum_l \\alpha_l \\alpha_j \\langle K(x_j,x), K(x_l,x)\\rangle_\\mathcal{H}\n",
    "$$\n",
    "using the reproducing property, $f(x)=\\langle K(x,\\cdot ), f(\\cdot) \\rangle$,\n",
    "$$\n",
    " \\sum_j \\sum_l \\alpha_l \\alpha_j K(x_j,x_l)\n",
    "$$\n",
    "or equivalently in matrix form ($K(j,l)=K(x_j,x_l)$)\n",
    "$$\n",
    "\\|f^*\\|^2_\\mathcal{H}=\\alpha^T K \\alpha\n",
    "$$\n",
    "\n",
    "## 11.1 Kernel regularized least squares\n",
    "\n",
    "Let us take one of the measures used to evaluate the fitting goodness of the function $f(x_i)$ i.e. $\\mathcal{L}(f(x_i),y_i)=(f(x_i)-y_i)^2$, then the problem to solve is\n",
    "\n",
    "$$f^*=\\underset{f\\in \\mathcal{H}}{\\operatorname{arg\\,min}}\\frac{1}{n}\\frac{1}{2}\\sum_i(f(x_i)-y_i)^2 +\\frac{1}{2}\\lambda\\|f\\|_{\\mathcal{H}}^2 $$\n",
    "\n",
    "The solution of the problem is given by the Representer's theorem:\n",
    "$f^*(x)=\\sum_j{\\alpha_j K(x_j,x)}$\n",
    "\n",
    "$$\n",
    "f^*=\\underset{f\\in \\mathcal{H}}{\\operatorname{arg\\,min}}\\frac{1}{2}\\sum_i{(f(x_i)-y_i)^2}+\\frac{\\lambda}{2}\\|f\\|_{\\mathcal{H}}^2\n",
    "$$\n",
    "\n",
    "Replacing the first term,\n",
    "\n",
    "$$\n",
    "\\alpha^*=\\underset{\\alpha_i \\in \\mathbb{R}}{\\operatorname{arg\\,min}}\\frac{1}{2}\\sum_i{(\\sum_j{\\alpha_j K(x_j,x_i)}-y_i)^2}+\\frac{\\lambda}{2}\\|f\\|_{\\mathcal{H}}^2\n",
    "$$\n",
    "replacing the norm and using the matrix form\n",
    "$$\n",
    "\\alpha^*=\\underset{\\alpha \\in \\mathbb{R}^N}{\\operatorname{arg\\,min}}\\frac{1}{2} \\|K\\alpha -y\\|^2+\\frac{\\lambda}{2}\\alpha^TK\\alpha\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\t\\alpha^*=\\underset{\\alpha \\in \\mathbb{R}^N}{\\operatorname{arg\\,min}}\\frac{1}{2} ( K\\alpha -y)^T( K\\alpha -y)+\\frac{\\lambda}{2}\\alpha^TK\\alpha\n",
    "\t$$\n",
    "\t\n",
    "Now we find the extrema by setting the gradient with respect to $\\alpha$ to $0$\n",
    "\\begin{align*}\n",
    "-K(K\\alpha -y)+\\lambda K\\alpha = 0\\\\\n",
    "(K+\\lambda I)\\alpha = y\\\\\n",
    "\\alpha = (K+\\lambda I)^{-1}y\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the solution is just a matrix inversion $\\mathcal{O}(n^3)$ and a matrix multiplication $\\mathcal{O}(n^2)$ that depends on the number of samples $n$ .\n",
    "The matrix $K + \\lambda I$ is symmetric positive definite, so the appropriate algorithm is Cholesky factorization ( or SVD ).\n",
    "\n",
    "RKHS needs the definition of a positive semidefinite kernel function $K$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\bf linear}: &K(x_i,x_j)=x_i^Tx_j\\\\\n",
    "{\\bf polynomial:}&K(x_i,x_j)=(x_i^Tx_j+1)^d\\\\\n",
    "{\\bf gaussian:}&K(x_i,x_j)=exp\\big( -\\frac{\\|x_i-x_j\\|^2}{\\sigma^2} \\big)\n",
    "\\end{align*}$$\n",
    "\n",
    "The **Gram matrix** shows the degree of shared information among training samples.\n",
    "\n",
    "**Gram matrix interpretation:**\n",
    "- **Bad Kernel:** Mostly diagonal $\\implies$ most points are orthogonal to each other, no clusters, no structure. \n",
    "- **Good Kernel:** The matrix has structure and show clusters.\n",
    "- **Ideal Kernel:** ${\\bf K_{\\mbox{ideal}} = yy^T}$ \n",
    "\n",
    "Using kernels can be seen as mapping data in an arbitrarily high dimensional space defined by the Gram matrix $K = \\Phi\\Phi^T$.\n",
    "\n",
    "\n",
    "> For a Gaussian kernel, with parameter $\\gamma = \\sigma$, as we decrease the value of $\\lambda$ for a fixed value of $\\gamma$ then the model fits better the data, providing a better classifier. This happens in a significant way when $\\gamma \\approx 1$. Observe we almost get a perfect classifier with this configuration. \n",
    "However, when $\\gamma<<1$, then as we decrease $\\lambda$ the kernel does not improve. Instead, it improves when increasing this value. \n",
    "\n",
    "Properties of kernels:\n",
    "Let $K_1$ and $K_2$ be valid Mercer's kernels, i.e. symmetric and positive definite matrices; and $\\alpha>0$, then the following are also valid kernels\n",
    "- $K(x_i,x_j)=K_1(x_i,x_j)+K_2(x_i,x_j)$\n",
    "- $K(x_i,x_j)=K_1(x_i,x_j)\\cdot K_2(x_i,x_j)$\n",
    "- $K(x_i,x_j)=\\alpha K_1(x_i,x_j)$\n",
    "**More ideas for designing your kernels**\n",
    "- Put your favorite distance $d(x_i,x_j)$ in $K(x_i,x_j)=e^{-d(x_i,x_j)}$. \n",
    "- Take your favorite nonlinear transform $\\Phi(x)$ and write\n",
    "$$K(x_i,x_j) = \\Phi(x_i)^T\\Phi(x_j)$$\n",
    "\n",
    "- Using kernels can be seen as mapping data in an arbitrarily high dimensional space spanned by $\\Phi(x)$.\n",
    "- The use of kernels allows a compact implicit description of the space generated by $\\Phi(x)$ by means of the Gram matrix $K(x,\\cdot) = \\Phi(x)\\Phi(\\cdot)$.\n",
    "- Observe that $K\\in \\mathbb{R}^{N\\times N}$. \n",
    "- The dimensionality of $\\Phi(x)$ for a gaussian kernel is $\\infty$.\n",
    "\n",
    "    \n",
    "The **kernel trick** replaces any inner product $<x_i,x_j>$ by a kernel $<x_i,x_j> = K(x_i,x_j)$. Historically, this was the only way of using kernels until 2006, when Oliver Chapelle considered the RKHS. This allowed to use this notion outside of the inner product reformulation.\n",
    "\n",
    "Remember that in the RKHS $f(x) = <K(x,\\cdot),f(\\cdot)>$ and that we can replace $f(x) = \\sum_i \\alpha_i k(x,x_i),$ $\\quad \\forall x_i \\in \\mathcal{D}_{train}$.\n",
    "\n",
    "> Kernels will project data into presumably very large dimensional spaces. If overfitting is the most serious problems in machine learning, the next most serious problem is high dimensional space data (specially in Instance Based Models). This problem has the proper name of **the curse of dimensionality**. \n",
    "This becomes a problem because low-dimensionality intuition does not wrok in high dimensions. \n",
    "The **kernel trick** replaces any inner product $<x_i,x_j>$ by the corresponding inner product in the reproducing kernel Hilbert space $K(x_i, x_j)$,: $<x_i,x_j> = K(x_i,x_j)$.\n",
    "For example, for the Instance Based Learning with classifier\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^N \\nu_i y_i x_i^Tx\n",
    "$$\n",
    "then the **Kernelized version of the classifier** is given as follows:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^N \\nu_i y_i K(x_i,x)\n",
    "$$\n",
    "\n",
    ">Our decision function is $ y_i(w^Tx_i) > 0$ but we seek to change it by $\\sum \\alpha_i k_i x_i$. sv machine\n",
    "In the case the cost function is 1, we just look for the feasible set. \n",
    "\n",
    "## 11.2 Online kernel perceptron\n",
    "\n",
    "A **perceptron** is a linear model that solves in a very simple way the problem\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}&\\quad 1\\\\\n",
    "\\text{subject to}& \\quad y_i(w^Tx_i) > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Remember that his problem is not part of the disciplined convex optimization family since strict inequalities are not handled.\n",
    "\n",
    "One way for solving this problem is to use **alternating projections**. This is, finding one violated constraint and project into the feasible set.\n",
    "\n",
    "In this sense, the algorithm can be written as follows\n",
    "\n",
    "<ol>\n",
    "<li>Select a random training sample pair $(x_i,y_i)$</li>\n",
    "<li>If $y_i(w^Tx_i)<0$ then $w_t \\leftarrow w_{t-1} + \\eta y_i x_i$ </li>\n",
    "</ol>\n",
    "\n",
    "The model is given by $f(x) = w^t x$.\n",
    "\n",
    "In standard perceptron, $\\eta = 1$.\n",
    "\n",
    "In order to understand this algorithm let us consider what is the projection of a sample in the feasible set:\n",
    "if $w_{t-1}$ does not satisfy the constraint, then we have to move in the direction of the normal of the hyperplane, $y_i x_i$ in order to get into the positive side, thus,  $w_t \\leftarrow w_{t-1} + \\eta y_i x_i$ is closer to satisfy the constraint.\n",
    "\n",
    "Let us change this to accomodate kernels in the primal.\n",
    "\n",
    "<div class = \"alert alert-info\">**Kernels in the primal vs dual**\n",
    "</div>\n",
    "\n",
    "Observe that $w = \\sum_{i=1}^l \\alpha_i {\\bf x}_i$\n",
    "\n",
    "With respect to $\\alpha$ the perceptron update rule becomes:\n",
    "\n",
    "$$\\sum_{i=1}^{l+1} \\alpha_i {\\bf x}_i=\\sum_{i=1}^l \\alpha_i {\\bf x}_i + y_i {\\bf x}_i \\implies \\alpha_i \\leftarrow \\alpha_i +y_i$$\n",
    "\n",
    "Thus we can rewrite the algorithm as the **dual algorithm**\n",
    "\n",
    "<ol>\n",
    "<li>Select a random training sample pair $(x_i,y_i)$</li>\n",
    "<li>If $y_i \\sum_{k=1}^l \\alpha_k \\langle{\\bf x}_k, {\\bf x}_i \\rangle < 0: \\quad  \\alpha_i \\leftarrow\\alpha_i +y_i$\\\\\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "And the model becomes: $f(x) =\\sum_{k=1}^l \\alpha_k \\langle{\\bf x}_k, {\\bf x}\\rangle$ \n",
    "\n",
    "**Generalized algorithm:**\n",
    "    \n",
    "<ol>\n",
    "<li>Select a random training sample pair $(x_i,y_i)$</li>\n",
    "<li> If $y_i f({\\bf x}_i) < 0: \\quad  \\alpha_i \\leftarrow \\alpha_i + y_i$</li>\n",
    "</ol>\n",
    "\n",
    "**Model: $f(x)$**\n",
    "\n",
    "Remember that $f(x) = K({\\bf X},x)^T\\alpha$ in RKHS. Thus, the **perceptron kernel algorithm** is given as follows\n",
    "<ol>\n",
    "<li>Select a random training sample pair $(x_i,y_i)$</li>\n",
    "<li> If $y_i \\sum\\limits_{j=1}^N K(x_j,x_i) \\alpha_j < 0: \\quad  \\alpha_i \\leftarrow \\alpha_i +y_i$</li>\n",
    "</ol>\n",
    "Where the model is $f(x) = \\sum_{j=1}^N K(x_j, x) \\alpha_j = K(X, x)^t \\alpha $\n",
    "\n",
    "We have no object function, we just work with the constraints.\n",
    "\n",
    "Consider now the Hinge loss: $max(0, 1-y_i(a^T x_i +b))$, using $max(0, 1-y_i(a^T k(x_i, X)))$, where $X$ is a matrix with the full training set\n",
    "\n",
    "Individual updates considering one data at a time are attractive since they naturally handle large scale data sets.\n",
    "A naive online version of kernel perceptron:\n",
    "- **Data** A pair $(x,y)$\n",
    "- **Result** A model $\\{\\alpha, sv\\}$, where $sv$ is the set of suppor vectors, $\\alpha$ the weighting vector. \n",
    "The algorithm consists of:\n",
    "1. Compute the vector $K(x, sv)$. \n",
    "2. If $yK\\alpha < 0$, then if $x \\in sv$ then $i \\leftarrow idx(x\\in sv)$, and $\\alpha_i \\leftarrow \\alpha_i +y$. Else $sv \\leftarrow sv \\cup x$ and $\\alpha \\leftarrow \\alpha \\cup y$. \n",
    "\n",
    "## 11.3 Online kernel SVM\n",
    "\n",
    "In a similar fashion we can extend support vector machines to the online case. \n",
    "\n",
    "In this case we are going to use subgradient methods. Remember that the classic support vector machine formulation is a follows,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}& \\quad\\|w\\|_2^2+C\\sum_i (1-y_i w^Tx_i)_+\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we now replace $f(x_i) = w^Tx_i$ by $f(x_i) = \\alpha_i k(x_i,{\\bf X})$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}& \\quad\\|{\\bf \\alpha}\\|_2^2+C\\sum_i (1-y_i {\\bf \\alpha}^T k(x_i,{\\bf X}))_+\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and its gradient:\n",
    "\n",
    "$$\\nabla_{\\alpha} F_{objective}= \\left\\{ \\begin{align} 2\\alpha - C y_i k(x_i,{\\bf X}) & \\quad \\text{if} \\; 1-y_i {\\bf \\alpha}^T k(x_i,{\\bf X})\\geq 0\\\\2\\alpha &\\quad \\text{otherwise} \\end{align}\\right. \n",
    "$$\n",
    "\n",
    "The former online version of SVM is not correct, though it is the most found formulation. The correct formulation replaces $\\ell_2$ norm by the Hilbert norm as follows,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}& \\quad\\|f\\|_\\mathcal{H}^2+C\\sum_i (1-y_i f(x_i))_+\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Instead of using $\\alpha^T \\alpha$ like in the previous case, now we use the norm, as wished from the beggining, using $\\alpha^T k \\alpha$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09974e27-22c6-4795-aeec-4b10e685cf36",
   "metadata": {},
   "source": [
    "# 12. Ensemble learning\n",
    "\n",
    "**Definition:** *Ensemble learning is based on the aggregation of decisions of different classifiers. For ensemble learning to work classifiers must be diverse.*\n",
    "\n",
    "## 12.1 Decision trees\n",
    "\n",
    "**Decision trees** are another kind of intutive **classification strategy based on the divide and conquer paradigm**. \n",
    "\n",
    "The basic **idea** in decision trees is to partition the space in patches and fit a model in that patch. There are two questions to answer in order to implement this solution:\n",
    "\n",
    "+ How do we partition the space?\n",
    "+ What model to use in each patch?\n",
    "\n",
    "In classification trees the second question is straight forward, each patch is given the value of a label and all data falling in that part of the space will be predicted as such.\n",
    "\n",
    "Elements of the **decision treen modelling:**:\n",
    "\n",
    "- Splits using axis-orthogonal hyperplanes. This is the key that allows \"interpretability\" of the results.\n",
    "\n",
    "- At each internal node we test a value of a feature. A feature and a threshold are stored for each internal node.  \n",
    "\n",
    "- Leaves makes the class prediction. If leaves are pure, we have to store the class label. If leaves are impure, then the fraction of samples for each class is stored and its frequency is returned when queried.\n",
    "\n",
    "\n",
    "What is great about decision trees?\n",
    "\n",
    "+ Trees are easy for humans to interpret. It can be seen as a set of rules. Each path from root to one leaf of the tree is an AND combination of the thresholded features.\n",
    "+ Given a finite data set, decision trees can express any function of the input attributes. In ${\\bf R}^d$ we can isolate every point in the data set by constructing a box around each of them.\n",
    "+ There can be more than one tree that fits the same data. From all of them we would like a tree with minimum number of nodes. But the problem is NP.\n",
    "\n",
    "###  Learning the tree\n",
    "\n",
    "Because the problem is NP we can resort to a greedy construction algorithm. **Greedy algorithms choose the current best binary partition without taking into account its impact on the quality of subsequent splits**.\n",
    "\n",
    "The algorithm idea is as follows:\n",
    "\n",
    "+ Initialize the algorithm with a node associated to the full data set. \n",
    "\n",
    "**while** the list is not empty\n",
    "1. Retrieve the first node from the list.\n",
    "2. Find the data associated to that node.\n",
    "3. Find a splitting point.\n",
    "4. If the node is splittable, create the nodes linked to the parent node and put them in the exploration list.\n",
    "\n",
    "#### The splitting criterion\n",
    "\n",
    "There are many different splitting criteria. The most common ones are:\n",
    "\n",
    "+ Misclassification error\n",
    "+ Gini index\n",
    "+ Cross-entropy/Information gain/Mutual information\n",
    "\n",
    "Withouth going into details, **misclassification error splits greedily selecting the split that corrects more data at each point**. **Gini index** and **cross-entropy** probabilistically model the notion of impurity of a node. The split is chosen so that the **average purity of the new nodes is maximized**. Observe that as we descend in the tree the purity increases and eventually converge to pure leaves. A nice way of thinking about entropy is Pedro Domingos' simile with surprise. Entropy measures the average surprise/information a probabilistic result yields. In a binary variable, the maximum surprise occurs when both outcomes are equally probable, one has the maximum uncertainty on the result. Otherwise, the surprise decreases. This behavior is also display in Gini's index.\n",
    "\n",
    "### Trees and overfitting\n",
    "\n",
    "**Because trees are very expressive models they can model any training set perfectly and easily overfit**.\n",
    "\n",
    "There are two ways of avoiding overfitting in trees:\n",
    "\n",
    "+ Stop growing the tree when the split is not statistically significant.\n",
    "+ Grow a full tree and post-prune.\n",
    "\n",
    "One of the simplest ways of post pruning is **reduced error prunning**. It goes like this,\n",
    "\n",
    "1. Split data into training and validation\n",
    "2. Create a candidate tree on the training set\n",
    "3. Do until further pruning is harmful\n",
    "    1. Evaluate impact on the validation set of removing each posible node (with descendants)\n",
    "    2. Greedily remove the node that improves the performance the most.\n",
    "    \n",
    "Pruning is not implemented in sklearn at this moment. \n",
    "\n",
    "## 12.2 Introduction to Ensemble learning\n",
    "\n",
    "Ensemble learning mimicks one of the human uncertainty reduction mechanism, seeking additional opinions before making a major decision.\n",
    "\n",
    "Ensemble learning is divided in two steps:\n",
    "\n",
    "1. Train a set of classifiers\n",
    "2. Aggregate their results\n",
    "\n",
    "There are different reasons for using ensemble learning in practice:\n",
    "\n",
    "1. **Statistical reasons:** The combination of outputs of different classifiers may reduce the risk of an unfortunate selection of a poorly performing classifier.\n",
    "2. **Large scale data sets:** It makes little sense to only have one classifier on very large sets of data. Partition data in smaller subsets and aggregate seems like a good idea.\n",
    "3. **Divide and conquer:** Some problems too difficult for a single classifier to solve. The decision boundary may be too complex or lie outside the space of functions of the classifier.\n",
    "4. **Data fusion:** Different source fusion is usually a problem. One usually faces data coming from heterogeneous sources and the question is how to fuse these data. One solution is to train one classifier per source and the fuse the decision of those experts.\n",
    "\n",
    "### Diversity\n",
    "\n",
    "One condition required for the system to work is that errors on different classifiers should be made on different samples in order for the strategic combination of the classifiers to correct possible errors in the judgement of the class o a particular instance. This effect has been called **diversity**.\n",
    "\n",
    "Diversity can be obtained in different ways:\n",
    "\n",
    "+ Using different training sets. Use resampling strategies to obtain different optimal classifiers. This effect is correlated with the notion of stability of the classifier and the concept of bias and variance of the classifier.\n",
    "+ Using different training parameters for different classifiers\n",
    "+ Combining different architectures. (i.e. svm, decission trees, ...)\n",
    "+ Training on different features. (i.e. random subspaces or random projections)\n",
    "\n",
    "### Different kinds of ensemble models\n",
    "\n",
    "From a topological and architectural point of view, we can find three great ways for building an ensemble:\n",
    "\n",
    "- **Bagging:** Horitzontal aggregation. All elements trained outputs are combined using some aggregation rule. In bagging the elements combined are trained independently. \n",
    "- **Boosting:** Horitzontal aggregation. Each component in the aggregation depends on all the others. In this kind of techniques the two most well known approaches for linking members are the notions of residual (gradient boosting) or statistical resampling (adaptive boosting). \n",
    "- **Stacking:** Vertical aggregation. In this kind of techniques the output of one member of the ensemble is the input for the next member. For example, each layer in a neural network is vertically stacked.\n",
    "\n",
    "### Bootstrapping aggregation \n",
    "\n",
    "Bootstrapping means **resampling the training data set with replacement**. Usually the same number of data as the original data set is used.\n",
    "\n",
    "**Bootstrapping aggregation (aka. Bagging)** is an ensemble technique that uses **multiple bootstrapped copies of the training set to build a set of classifiers**. One classifier for each bootstrapped training copy. And then, use a combination technique, such as majority voting, in order to take the final decision.\n",
    "\n",
    "Bagging performance improvement is due to the **reduction of the variance of the classifier while mantaining its bias**.\n",
    "\n",
    "## 12.3 Random forest\n",
    "\n",
    "**Definition:** *Random Forest technique introduces a randomization over the feature selected for building  each tree in the ensemble in order to improve diversity in an attempt to reduce variance evan more. Let us code this variant of bagging.*\n",
    "\n",
    "\n",
    "## 12.4 Incremental learning: stage-wise approximation and boosting techniques\n",
    "\n",
    "Up to now we have seen optimization techniques based on full step optimisation (e.g. gradient descent approaches). This is, we optimise all members of the ensemble at each step. We will now consider **stage-wise approximations** or incremental approximations. The most simple approach of this family is a greedy stage-wise approximation. In stage-wise approximations *we consider the model up to our current time $t$ as fixed, and we just add and optimize a new element of the ensemble.*\n",
    "\n",
    "**Gradient boosting techniques:**\n",
    "\n",
    "Our model is\n",
    "\n",
    "$$H_t(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$$\n",
    "\n",
    "Consider for example a least square approach \n",
    "\n",
    "$$\\mathcal{L}(x,y) = \\sum_{j=1}^N (y_j - \\sum_{t=1}^T \\alpha_t h_t(x_j))^2$$\n",
    "\n",
    "at time $t+1$, we have to select $h_{t+1}$ and optimize $\\alpha_{t+1}$ while keeping the model up to that point constant.\n",
    "\n",
    "$$H_{t+1}(x) = \\sum_{t=1}^T \\alpha_t h_t(x) + \\alpha_{t+1}h_{t+1}(x)$$\n",
    "\n",
    "If we plug this model into the loss \n",
    "\n",
    "$$\\mathcal{L}(x,y) = \\sum_{j=1}^N (\\underset{r_t(x_j)}{\\underbrace{y_j - \\sum_{t=1}^T \\alpha_t h_t(x_j)}} + \\alpha_{t+1}h_{t+1}(x_j))^2$$\n",
    "\n",
    "where $r_t(x_j) = y_j - H_t(x_j)$ is the error or residue commited by the model at time $t$ on example $x_j$.\n",
    "\n",
    "In this simple case, we have to solve for $\\alpha_{t+1}$ and $h_{t+1}$,\n",
    "\n",
    "$$\\nabla_{h_{t+1}} \\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))^2 = 2\\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))\\alpha_{t+1} = 0$$\n",
    "\n",
    "$$\\nabla_{\\alpha_{t+1}} \\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))^2 = 2\\sum_{j=1}^N (r_t(x_j) + \\alpha_{t+1}h_{t+1}(x_j))h_{t+1}(x_j) = 0$$\n",
    "\n",
    "From the second equation we can see that\n",
    "\n",
    "$$\\sum_{j=1}^N (r_t(x_j)h_{t+1}(x_j) + \\alpha_{t+1}h_{t+1}(x_j)h_{t+1}(x_j)) = $$\n",
    "$$r^T_t(x)h_{t+1}(x) + \\alpha_{t+1}h^T_{t+1}(x)h^T_{t+1}(x) = 0$$\n",
    "\n",
    "Observe that $\\alpha_{t+1}$ is proportional to the inner product of the current residue and the candidate function, \n",
    "\n",
    "$$\\alpha_{t+1} = \\frac{r^T_t(x)h_{t+1}(x)}{h^T_{t+1}(x)h_{t+1}(x)}$$\n",
    "\n",
    "In the *case we have a binary problem* $h^T_{t+1}(x) \\in \\{-1,+1\\}$, $h^T_{t+1}(x)h_{t+1}(x) = N$, the number of samples.\n",
    "\n",
    "*In this case the selection of the optimum $h$ corresponds to the one that has minimum error.*\n",
    "\n",
    "Another popular example of boosting is **Adaboost** (Adaptive boosting)\n",
    "\n",
    "In Adaboost we use the same idea but we change the loss function and use \n",
    "\n",
    "$$\\mathcal{L}(x,y) = \\sum_i e^{-y_iH(x_i)}$$\n",
    "\n",
    "## 12.5 Multi-class problem (reductionist frameworks)\n",
    "\n",
    "There are very few models that intrinsically handle the multi-class case. By intrinsically handling the multi-class problem I am referring to methods in which we do not have to worry about how many classes our problem has. The two big families of models that can deal with the problem are\n",
    "\n",
    "+ Decision trees: the leaves encode the class.\n",
    "+ Nearest Neighbors: we only care about class labels of instances close to my query sample.\n",
    "\n",
    "What about the rest of the models? Did not Bayesian models or Neural Networks also handle this problem? Yes, they work in the multi-class case. But *we have to worry about how many classes there are*. In particular *we have to build a model for each class and then take a maximum score/probability/confidence among the predictions*. This way of addressing the multiclass problem is also known as **one-against-all** because *we consider one model for each class while the samples from the rest of the classes are considered as negative samples*. This is the first example of a reductionist framework.\n",
    "\n",
    "The **reductionist framework** refers to those *ensemble methods that allow to reduce the multi-class problem to a set of binary problems*. In a $K$ class problem, the two most common approaches in this framework are:\n",
    "\n",
    "+ **one-against-all:** We consider $K$ partitions of the problem, corresponding to setting one class as positive class and the rest as negative. \n",
    "+ **one-against-one:** We consider all posible pairs of classes and build a model for each subproblem. \n",
    "\n",
    "## 12.6 Error correcting output coding\n",
    "\n",
    "Error correcting output coding is a generalization of the methods shown before. In the most general case each class is assigned a ternary code $c_i \\in \\{+1,0,-1\\}^l$ with length $l$. This step is called **coding**. In testing a new sample will be given a test code and this will be compared according to some distance to the class codewords. The class with the closest codeword will be selected as the predicted class. This step is called **decoding**.\n",
    "\n",
    "### Understanding the coding step\n",
    "\n",
    "If we arrange the codewords as rows in a matrix we obtain the coding matrix $M \\in \\{+1,0,-1\\}^{K\\times l}$. Consider the following example with four classes and code length $l=3$:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>$h_1$</th>\n",
    "    <th>$h_2$</th>\n",
    "    <th>$h_3$</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The first class, $y_1$ is coded as $(1,1,1)$, the second $y_2$ is coded as $(1,-1,0)$, and so on.\n",
    "\n",
    "Note that the columns of the matrix define a binary problem involving all the classes in the following way: in same column, all classes with code $+1$ belongs to the same meta-class, all classes with code $-1$ to the other meta-class, and all classes with code $0$ are not considered in that particular problem. In our example, the first column defines a binary problem involving the discrimination of all samples from classes $y_1,y_2$ (coded as $+1$) against all the samples of $y_3,y_4$ (coded as $-1$). The second column only considers the samples of class $y_1$ against the samples of class $y_2$. Note that all the zero coded classes are not considered. \n",
    "\n",
    "Given a coding matrix a classifier is trained for each column according to the column defined binary problem. \n",
    "\n",
    "### Understanding the decoding step\n",
    "\n",
    "Given a set of classifiers trained according to the problems defined by the columns of the coding matrix, in the prediction step all the classifiers are applyed to the testing sample. As a result a binary code $t$ is obtained. This coded is compared to all the class codes according to some decoding/distance metric. The most common ones are:\n",
    "\n",
    "+ Hamming decoding/$\\ell_1$-decoding\n",
    "$$d(a,b) = \\frac{1}{2}\\sum\\limits_{i=1}^l |a_i-b_i|$$\n",
    "\n",
    "+ Euclidean decoding\n",
    "$$d(a,b) = \\sqrt{\\sum\\limits_{i=1}^l (a_i-b_i)^2}$$\n",
    "\n",
    "For example, consider that $t=(-1,-1,-1)$. Note that there is no exact code in the coding matrix, thus we have to check for the closest one. If we apply Hamming decoding we obtain\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Hamming</th>\n",
    "    <th>Euclidean</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$3$</th>\n",
    "    <th>$\\sqrt{12}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$\\frac{3}{2}$</th>\n",
    "    <th>$\\sqrt{5}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$\\frac{3}{2}$</th>\n",
    "    <th>$\\sqrt{5}$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$\\frac{1}{2}$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Observe that in both cases the sample will be predicted as class $y_4$.\n",
    "\n",
    ">One vs one:\n",
    "We have 1 and -1 for the two we are comparing and 0 for the remaining classes. \n",
    "One vs all:\n",
    "We have 1 on the one separated, and -1 on all the other classes. \n",
    "\n",
    "One vs all:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>$h_1$</th>\n",
    "    <th>$h_2$</th>\n",
    "    <th>$h_3$</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "One vs one:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>$h_1$</th>\n",
    "    <th>$h_2$</th>\n",
    "    <th>$h_3$</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>$y_1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "    <th>$1$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_2$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$0$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_3$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$-1$</th>\n",
    "    <th>$0$</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>$y_4$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$0$</th>\n",
    "    <th>$-1$</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> The decision boundary in the hard code is less linear and adjust the model in a way that looks like all the points are predicted with the same precision. It looks like it tries to adjust and equalize the distance each point from a class has to the decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab652df8-49c9-40cc-965c-a49f68b0ceb7",
   "metadata": {},
   "source": [
    "## 14.a) Manifold and representation learning:\n",
    "\n",
    "**Definition**: Non-linear dimensionality reduction is also called **manifold learning**. The basic hypothesis in manifold learning is that data lies in a manifold of intrinsic dimensionality smaller than the dimensionality of the original space. A nice example of a manifold is the well-known S-curve.\n",
    "\n",
    "### Multidimensional Scaling (MDS):\n",
    "\n",
    "**Definition**: Metric MDS attempts to construct a constellation of points in Euclidean space by using information about the distances between the samples. Formally, it **optimizes Kruskal's stress equation**,\n",
    "\n",
    "$$\\underset{x'}{\\text{minimize}} \\quad\\sum\\limits_{j=1}^N\\sum\\limits_{i=1}^N \\frac{(d_{i,j}-d'_{i,j})^2}{d_{i,j}^2} $$\n",
    "\n",
    "where $d_{i,j} =\\|x_i-x_j\\|^2$ and $d'_{i,j} =\\|x'_i-x'_j\\|^2$, with $x\\inÂ {\\bf R}^K$,  $x'\\in {\\bf R}^k$, $K>k$. The solution to this problem when distances are Euclidean is exactly the same as Principal Component Analysis. The value of this method is that it is **defined for non Euclidean metric and in non-metric cases**, where one attempts to have a good visualization.\n",
    "\n",
    "The result is a mapping of the data that can be used for visualization but the application for mapping purposes requires recomputing the embedding.\n",
    "\n",
    "### Isomap:\n",
    "\n",
    "**Definition**: Isomap is an extension of MDS. The main idea is to perform MDS, not in the original space, but in the **geodesic space** of the nonlinear data manifold. The **geodesic distances represent the shortest paths along the curved surface of the manifold measured as if the surface were flat**. This can be approximated by a sequence of short steps between neighbouring sample points. Isomap then applies MDS to the geodesic rather than straight line distances to find a low-dimensional mapping that preserves these pairwise distances.\n",
    "\n",
    "1. Construct a neighborhood graph. \n",
    "2. Compute shortest path (the geodesic distances) between all points.\n",
    "3. Embed the data via MDS so as to preserve these distances.\n",
    "\n",
    "Isomap can be used as a mapping technique.\n",
    "\n",
    "### Locally linear Embedding:\n",
    "\n",
    "**Definition:** **Locally linear embedding** addresses the problem of non-linear dimensionality reduction. It is based on **linearly approximating the manifold around each sample and then finding the low dimensional representation that best reconstruct that sample according to the linear manifold**. \n",
    "The algorithm roughly is as follows\n",
    "\n",
    "1. Find the neighbours of each data sample $x_i$.\n",
    "2. Compute the weights that best linearly reconstruct $x_i$ from its neighbours.\n",
    "3. Find a low-dimensional embedding vector $x'_i$ that is best reconstructed by the weights determined in the previous step.\n",
    "\n",
    "One can apply the local manifold found in the **fitting procedure to new data** but it is advisable **not to use this kind of methods in a classification pipeline**.\n",
    "\n",
    "### T-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "**Definition**: **Stochastic Neighbor Embedding (SNE)** changes the **notion of distance to that of conditional probabilities**. The **similarity** of a data point $x_i$ to $x_j$ is modelled by the **conditional probability** $p_{i|j}$ that $x_i$ would select $x_j$ as its neighbors **if neighbors were selected in proportion to their pdf under a Gaussian centered at $x_j$**. The similarity in the reduced space $q_{i|x}$ is modeled in the same way. \n",
    "\n",
    "$$p_{i|j} = \\frac{e^{-\\|x_i-x_j\\|^2/2\\sigma^2}}{\\sum\\limits_{j\\neq k}e^{-\\|x_j-x_k\\|^2/2\\sigma^2}}$$\n",
    "and \n",
    "$$q_{i|j} = \\frac{e^{-\\|y_i-y_j\\|^2/2}}{\\sum\\limits_{j\\neq k}e^{-\\|y_j-y_k\\|^2/2}}$$\n",
    "\n",
    "And this method attemps to **minimize the mismatch between the distributions** using **Kullback-Leibler divergence** over all data points:\n",
    "\n",
    "$$\\text{minimize} \\sum_i\\sum_j p_{i|j} \\log \\frac{p_{i|j}}{q_{i|j}} $$\n",
    "\n",
    "The notion of similarity in this definition is directional. This can produce some artifacts that can be circumvented using a symmetric approach as follows \n",
    "$$p_{ij} = \\frac{p_{i|j}+p_{j|i}}{2}$$\n",
    "\n",
    "**Definition:** **T-SNE** modifies this **symmetric SNE** by changing the Gaussian distribution in the projected space by a **t-distribution**, i.e.\n",
    "\n",
    "$$q_{i|j} = \\frac{(1-\\|y_i-y_j\\|^2)^{-1}}{\\sum\\limits_{j\\neq k}{(1-\\|y_j-y_k\\|^2)^{-1}}}$$\n",
    "\n",
    "The minimization is done using **gradient descent**. The parameter $\\sigma$ is fixed by setting the perplexity value (i.e. $e^H_{ij}$) that can be interpreted as the soft version of the number of neighbors. Values between 5 an 50 are suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d35396-7846-46c8-b233-36b087737066",
   "metadata": {},
   "source": [
    "# 14.b) Unsupervised learning\n",
    "\n",
    "## Autoencoders:\n",
    "\n",
    "**Definition:** they work by **compressing the data**, this is, we have a **bottlenck**, which is a good representation for reconduction purposes.\n",
    "\n",
    "With a neural network we have too ways to build an autoencoder:\n",
    "- **Physical bottleneck (BN)**: literally have a set of layers that decrease towards a small layer in the middle reduced layer, which has the necessary information to decode all the data, the **encoder** part >, and then the dimension of the layers increase again, the **decoder** part <. Then, the layer in the middle has the necessary information to recover all the data. We have therefore a compression of the data and decompression into the output data, ><. \n",
    "- **Logical bottleneck (BN)**: the inverse model, in which we have an expansion and then a compression <>. In this case we introduce **spartion** (most of the neurons in the middle are zero) and that is intereseting because it is used in sparse dictionaries.\n",
    "\n",
    "### Physical bottleneck\n",
    "\n",
    "Interesting property of Autoencoder: let's suppose the input data is patient health data. Then we use an encoder, getting a middle reduced layer, which has the necessary information to decode all the data. Imagine we supress the decoder part. We end up with a comprimed domain with all the information relevant for restoring the data, and we have lost the identity transformation. This is called **deep patient**. It was used in order to treat hundreds of deseases. It worked better using the reduced representation. Additionally, it is **pseudo-anonymised**. This is important when facing the protection regulation, because by pseudo-anonymising the data it does not apply. To be anonymous means that there is no way to recover the original data, since with just the compressed data we cannot go back to the original data, unless we have the decoder (we have erased it!). \n",
    "\n",
    "Here it is interesting to note that the middle layer neurons are have enough information to reconstruct all the input data, however we do need all the middle neurons (which could be 2 neurons, compared to an input of dimension 64) to reconstruct the input information. \n",
    "\n",
    "### Logical bottleneck\n",
    "\n",
    "Now we come from a large range of neurons, so when we need to reconstruct data we do not need all of them, now we have the structure <>, meanwhile before we had the model structure ><. Now, we want the high dimensional layer to be a bottleneck, so consider the middle layer loss function\n",
    "$$\n",
    "\\mathcal{L} = \\sum_k \\sum_{i,j=1}(x_{ij}^k - \\hat{x}_{ij}^k)^2 + \\lambda || r ||_1\n",
    "$$\n",
    "We have the situation in which the input is $x \\in \\mathbb{R}^{64}$, in the middle we have the layer with $r \\in \\mathbb{R}^{256}$ neurons, then we apply a sparse coding, adding some zeros in the neurons which do not contain certain information, and then our output is $\\hat{x} \\in \\mathbb{R}^{64}$. \n",
    "\n",
    "**Note then that in this case some neurons are not important nor used for coding, meanwhile in the previous approach, we need all of the neurons of the middle layer, because they have already been reduced and we do need them all to decode the information we had**. \n",
    "\n",
    "\n",
    "### Variational Autoencoders\n",
    "\n",
    "Consider the process: $x \\rightarrow x + \\eta \\rightarrow \\boxed{I} \\rightarrow \\hat{x}$, where $\\eta$ is a noise. Remember the autoencoder consists of an Encoder and an decoder, so we have a redutction of the data using the encoder, until we get enough information to reconstruct the initial data, and then we apply the decoder to get the output. \n",
    "\n",
    "For the following computations we can assume that $q(x_1, x_2, x_3)= q_1(x_1) q_2(x_2) q_3(x_3)$, where $q_i \\sim N(x_i, \\sigma)$. \n",
    "\n",
    "Given the encoder > **we assume that the outside representation is a gaussian**, we are forcing that the output in the domain is a gaussian distribution. We are using **probabilistic representation** of the data. We are using **Variational Autoencoders to smooth the manifolds in the middle**, so then we have some variability around the variables. \n",
    "\n",
    "This **probabilistic behavior requires the use of a different loss, Kullbakc-Leibler divergence.**\n",
    "\n",
    "\n",
    "We will use standard normal distribution, resulting in the following loss:\n",
    "\n",
    "$$L (x,\\hat{x}) = \\|x-\\hat{x}\\| + KL(\\mathcal{N}(\\mu(x),\\sigma(x)),\\mathcal{N}(0,1))$$\n",
    "\n",
    "We are using **Variational Inference for learning**. This is we are **approximating the desired distribution $p(z|x)$ by a controlled set of base functions $q(z)$**. Usually, we use mean field approximation in which **we assume that all variables in $z$ are independent and Gaussian**. This is $q_x(z) = \\prod q_x(z^{(i)}) = \\prod \\mathcal{N}(\\mu^{(i)}(x),\\sigma^{(i)}(x))$.\n",
    "\n",
    "### Generating adversarial networks\n",
    "\n",
    "We have now a **uniform distribution** from which we get data $z \\in U()$ which we input in a model formed by a decoder < se we get $x'$ as the output expanded data. Now we introduce this data $x'$ that come from the generated data, and some real data $x$ which actually comes from the real data, into a Classifier, which we train in order to make it able to discriminate whether it is fake or not. Once we've done it, we freeze the classifier, then we train the decoder to produce data which the frozen classifier cannot identify. Once we've done it, we unfreeze the classifier, and freeze the decoder, in order to train the classifier to identify the fake data. \n",
    "\n",
    "We keep on iterating this process until we get a decoder that produces fake data which cannot be distinguished from the real data. \n",
    "\n",
    "$z \\sim U() \\rightarrow D < \\rightarrow x'$ and then ${x, x'} \\rightarrow \\boxed{C} \\rightarrow$ fake or not. \n",
    "\n",
    "This is a clever trick for **guiding a network into mimicking a sampled data distribution**. In a nutshell it consists of a two part process:\n",
    "\n",
    "- **Part 1: The generator** A network plays the role of a generator. We input data coming from one well-known distribution such as a Normal distribution and the output is expected to be the type of data we desire, e.g. cat images, words, etc. Just as we saw with the decoder of the Auto-encoder.\n",
    "\n",
    "- **Part 2: The discriminator** In order for the generator to be able to achieve its goal here comes the clever trick. A discriminator is trained with data coming from the true source we want to mimick and with the data coming from the generator. These are mixed. And the role of the discriminator is to tell apart real from fake data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab87c6-00d8-453a-a153-b5b81cd1f23c",
   "metadata": {},
   "source": [
    "# 16. Mixture models\n",
    "\n",
    "Clustering consists of making groups. \n",
    "\n",
    "Many clustering algorithms are related to inferences of multiple gaussians. Some approaches regarding clustering go from a very classic algorithm **k-means** to more interesting techniques, like Gaussian mixture models.\n",
    "\n",
    "There are two different aspects to take into account when *clustering*: \n",
    "- Compacity: based on how close the points are. Methods: \n",
    "    - Kmeans, hierarchical clusetering, soft k-means, mixture of gaussians\n",
    "- Connectivity: based on the notion of connection of the points. Methods:\n",
    "    - DBScans, Spectral clustering (using eigenvalues or eigenvectors to figure out the clustering), HDBScan (Hierarchical means we change the radius)\n",
    "   \n",
    "Then the clustering techniques are based on grouping partition unlabeled examples into disjoint subsets of clusters, such that:\n",
    "\n",
    "+ Examples within a cluster are similar (high intra-class similarity)\n",
    "+ Examples in different clusters are different (low inter-class similarity)\n",
    "\n",
    "It can help in discovering new categories in an unsupervised manner (no sample category labels provided).\n",
    "\n",
    "This task has many application in different domains:\n",
    "\n",
    "+ Group genes that perform the same function\n",
    "+ Group individuals that has similar political view\n",
    "+ Categorize documents of similar topics\n",
    "+ Market segmentation/ building customer profiles for market analysis\n",
    "+ Social network analysis\n",
    "+ Astronomical data analysis\n",
    "+ Compression \n",
    "+ Detection of near duplicates\n",
    "+ Grouping related stock quotes for investment portfolio management\n",
    "+ Building a code book of prototype samples for supervised learning algorithms\n",
    "\n",
    "## K-means:\n",
    "\n",
    "The K-means algorithm looks at the problem of identifying groups, or clusters of data points in a multidimensional space. Our goal is to partition the space in $K$ clusters. In k-means, the clusters are represented by $k$ prototypes $\\{m^{(k)}\\}$. We may think about these prototypes as the centers of the clusters. \n",
    "\n",
    "Let's suppose we have a generator of data that generates clusters of data. We have for example three different groups of data, three prototypes. Then we compute the distance to the prototype. Consider the areas of influence for each three points, then we compute the mean of the data points on each area of influence. Then we repeat the process for the new mean points. Compute the new means again and separate the 3 areas of influence. We keep iterating this process, until we get the 3 areas of influence that separates the 3 groups of data and we have fixed representers of the three clusters. \n",
    "\n",
    "K-means then consists of compressing groups into prototypes. It is important that the number of prototypes is fixed a priori, we are stating the number of clusters before. \n",
    "\n",
    "Our goal is twofold:\n",
    "+ Find the values of the prototypes.\n",
    "+ Find the assignment of the points in the space given the prototypes.\n",
    "\n",
    "Let us model these two quantities. First, let us model the assignment rule by defining a **responsibility** variable, that tells us that a point belongs to a certain cluster,\n",
    "\n",
    "$$r_n^{(k)} = \\left\\{\\begin{align}1 & \\quad \\text{if}\\; x_n\\in c_k\\\\ 0 &\\quad \\text{otherwise} \\end{align}\\right.$$\n",
    "\n",
    "Now we can define the **objective function**. In k-means this is a measure that models the fact that elements in one cluster have small intra-cluster distance compared with the distance to elements in the rest of the clusters. We can formulate the objective as \n",
    "\n",
    "$$\\underset{r,\\mu}{\\text{minimize}}\\quad \\sum_{k=1}^K\\sum_{i=1}^N r_i^{(k)}\\|x_i-m^{(k)}\\|^2$$\n",
    "\n",
    "The solution of this problem can be done by iterating two steps, similar to a block coordinate optimization or an alternating projections technique, in which you solve first for one while keeping the other fixed, and then you optimize the other based on the former solution. \n",
    "\n",
    "In this case we may choose an initial value of the prototypes $\\{m^{(k)}\\}$ and then alternate the following steps:\n",
    "+ **Step E:** We minimize the objective with respect to $r_i^{(k)}$ keeping the values of the prototypes fixed.\n",
    "+ **Step M:** We minimize the objective with respect to $\\{m^{(k)}\\}$ keeping the values of the responsibilities fixed.\n",
    "\n",
    "This technique as explained in this context is a protoform of a more general approach commonly called **Expectation Maximization (EM)**, where the expectation step **E** and the maximization step **M** alternates.\n",
    "\n",
    "In the context of k-means, solving the **E** step corresponds to assign the points to the nearest prototype, thus\n",
    "\n",
    "$$r_n^{(k)} = \\left\\{\\begin{align}1 & \\quad \\text{if}\\; k = \\arg\\min_j \\|x_n-m^{(j)}\\|^2\\\\ 0 &\\quad \\text{otherwise} \\end{align}\\right.$$\n",
    "\n",
    "The **M** step correspondes to optimizing with respect to $\\{m^{(k)}\\}$, if we take derivatives and set them to zero, we obtain\n",
    "$$-2\\sum_{i=1}^N  r_i^{(k)}(x_i-m^{(k)})=0$$\n",
    "which gives\n",
    "\n",
    "$$m^{(k)} = \\frac{\\sum_n r_n^{(k)}x_n}{\\sum_i r_i^{(k)}}$$\n",
    "\n",
    "As denominator we have the number of elements in that area, and then we are accounting a weigth for each of them, obtaining the mean of the points in that area, which is the base of K-means. In this algorithm the loss function is clearly $\\sum_{k=1}^K \\sum_{i=1}^n r_i^{(k)} || x_i - c_k||^2 $, which we minimize in terms of $c_k, r_i^{(k)}$.\n",
    "\n",
    "> Note that we are making isotropical assumptions: all the clusters are the same size and shape (this is a mixture of spherical Gaussians, all of them with the same prior probability), however we can **improve the results** by considering different areas. Responsabilities create partitions, which we are forcing. We have therefore a disjoint space formed by partitions, this is when we classify a point in a cluster, it is discarted for all the other clusters. \n",
    "Consider now the notion of **similarity**, given by the responsability as defined susequently:\n",
    "This **responsibility strength** is based by the value of $\\beta$. It conditions the **rate of descent of the exponential**. And it describes the **soft k-means**.\n",
    "\n",
    "## Soft k-means:\n",
    "\n",
    "We set a **stiffness parameter** $\\beta$ and we change the assignment rule considering the similarity to each of the means. Observe that the **similarity is now defined with respect to all possible means**, this is each point resebles something to each mean. This will mean that all data contribute to the assignment.\n",
    "\n",
    "$$r_k^{(n)} = \\frac{e^{-\\beta d(x^{(n)},m^{(k)})}}{\\sum_i e^{-\\beta d(x_n,m_i)}}$$\n",
    "\n",
    "The sum of the responsibilities of the k means for each point adds to one.\n",
    "\n",
    "> If we fix the k to 3 we get a quite accurate results of the clustering. Considering the k=10 we also obtain a satisfactory result, cause all centroids converge to three or 4 final means. \n",
    "Using 50 we also get that the centroids converge into 3 or 4 final centroids. We have a large gap between 50 to 4. \n",
    "But changing the parameter k is not the same as changing the parameter $\\beta$. However if we change $\\beta$ then we may modificate the number of cluster we obtain. For example, if we increase $\\beta$ we may get less clusters. If I want clusters of smaller size I may get more clusters, and if I want them bigger, I'll get less. We can think of $\\beta = 1/\\sigma^2$.\n",
    "No matter how many times we run the previous problem, we always get the same results, since it is a stable problem. \n",
    "\n",
    "## Mixture of Gaussians: \n",
    "\n",
    "From all the former examples we can easily consider a mixture of Gaussian distributions in the following form,\n",
    "\n",
    "$$p({\\bf x}) = \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}({\\bf x}|\\mu_k,\\Sigma_k)$$\n",
    "\n",
    "Let us derive the mixture of Gaussians from a Bayessian perspective. This will also allow us to introduce our first latent variable and the depiction of joint probability density functions in terms of graphical models.\n",
    "\n",
    "Let us introduce a K-dimensional binary random variable ${\\bf z}$ with a 1-of-K representation ($z_i = 1$ if $i=k$ and $z_i = 0$ if $i\\neq k$, with $z_i\\in\\{0,1\\}$). Observe that this random variable has just K possible random states according to which component is nonzero. \n",
    "\n",
    "We can define the joint distribution $p({\\bf x},{\\bf z}) = p({\\bf x}|{\\bf z})p({\\bf z})$. \n",
    "\n",
    "Let us consider now the probability of $z_k$\n",
    "\n",
    "$$p(z_k=1) = \\pi_k$$\n",
    "\n",
    "where $0\\leq \\pi_k \\leq 1$, and $\\sum_k p(z_k=1) = 1$.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$$p({\\bf x}|z_k=1) = \\mathcal{N}({\\bf x}|\\mu_k,\\Sigma_k)$$\n",
    "\n",
    "Let us compute now the marginal distribution with respect to $x$, i.e. $p({\\bf x}) = \\sum_z p({\\bf x}|{\\bf z})p({\\bf z})$, this is\n",
    "\n",
    "$$p({\\bf x}) = \\sum\\limits_{k=1}^K p(z_k=1)p({\\bf x}|z_k=1)$$\n",
    "\n",
    "Note that we are computing this value by considering all states of random variable $z_k$.\n",
    "\n",
    "$$p({\\bf x}) = \\sum\\limits_{k=1}^K \\pi_k\\mathcal{N}({\\bf x}|\\mu_k,\\Sigma_k)$$\n",
    "\n",
    "Great, we have just derived the same as we got at the starting point. But what is this good for? We will see in a minute that this will simplify our computational procedure.\n",
    "\n",
    "With this representation we can ask about the posterior probability of the assignment state variable, i.e.\n",
    "\n",
    "$$p(z_k=1|{\\bf x}) = \\frac{p(z_k=1)p({\\bf x}|z_k=1)}{\\sum\\limits_{i=1}^K p(z_i=1)p({\\bf x}|z_i=1)}$$\n",
    "\n",
    "$$ = \\frac{\\pi_k\\mathcal{N}({\\bf x}|\\mu_k,\\Sigma_k)}{\\sum\\limits_{i=1}^K \\pi_i\\mathcal{N}({\\bf x}|\\mu_i,\\Sigma_i)}$$\n",
    "\n",
    "Observe that this quantity may be considered as the responsibility of that particular Gaussian distribution for generating/\"explaining\" observation ${\\bf x}$.\n",
    "\n",
    "### Maximum Likelihood\n",
    "\n",
    "The maximum likelihood objective considering i.i.d. samples corresponds to the problem of \n",
    "\n",
    "$$\\text{maximize}\\quad \\prod\\limits_{i=1}^N p(x_i|\\pi,\\mu,\\Sigma)$$\n",
    "\n",
    "we can equivalently maximize the log probability,\n",
    "\n",
    "$$\\text{maximize}\\quad \\sum\\limits_{i=1}^N \\ln \\{\\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}({x_i|\\mu_k,\\Sigma_k})\\}$$\n",
    "\n",
    "> In order to compute the maaximum likelihood of the loss function we have to make some assumptions, such as the pdf are from the exponential family of functions! Then we maximize the joint probability.\n",
    "\n",
    "We may use standard gradient based techniques, or use the alternating projections technique. In this case we will use expectation maximization.\n",
    "\n",
    "Let us write down the conditions for solving that problem: \n",
    "\n",
    "- With respect to the means $\\mu_k$\n",
    "$$0 = \\sum\\limits_{k=1}^N\\underset{\\gamma(z_n^{(k)})}{\\underbrace{\\frac{\\pi_k\\mathcal{N}({\\bf x}|\\mu_k,\\Sigma_k)}{\\sum\\limits_{i=1}^K \\pi_i\\mathcal{N}({\\bf x}|\\mu_i,\\Sigma_i)}}} \\Sigma^{-1}_k(x_n-\\mu_k)$$\n",
    "\n",
    "$$0 = \\sum\\limits_{k=1}^N \\gamma(z_n^{(k)}) \\Sigma^{-1}_k(x_n-\\mu_k)$$\n",
    "\n",
    "if $\\Sigma$ is non singular we can rearrange terms to obtain,\n",
    "\n",
    "$$\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_n^{(k)})x_n$$\n",
    "\n",
    "with \n",
    "\n",
    "$$N_k = \\sum_{n=1}^N \\gamma(z_n^{(k)})$$\n",
    "\n",
    "with respect to $\\Sigma$,\n",
    "\n",
    "$$\\Sigma_k = \\frac{1}{N_k}\\sum_{n=1}^N \\gamma(z_n^{(k)})(x_n-\\mu_k)(x_n-\\mu_k)^T$$\n",
    "\n",
    "Finally, with respect to $\\pi_k$,\n",
    "\n",
    "$$\\pi_k = \\frac{N_k}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286721c6-9aec-46a7-b7fa-6c15d34dc630",
   "metadata": {},
   "source": [
    "# ToDo:\n",
    "\n",
    "- Pill 10: linear model: \n",
    "    - Summary: paper done \n",
    "    - Quiz: there is no review available, check Leo's\n",
    "- Pill 11: Kernels\n",
    "    - Summary:\n",
    "    - Quiz: reviewed\n",
    "- Pill 12: Ensemble models:\n",
    "    - Summary: \n",
    "    - Quiz: to do (24/12) !\n",
    "- Pill 13: neural network and deep learning\n",
    "    - Summary: paper done\n",
    "    - Quiz: \n",
    "- Pill 14: Unsupervised learning\n",
    "    - Summary: done\n",
    "    - Quiz: \n",
    "- Pill 15: Dimensionality Reduction and Gaussian Discrimination: does not enter in the exam\n",
    "- Pill 16: Mixture Models and Clustering:\n",
    "    - Summary: done\n",
    "    - Quiz: there is no quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ced1d-11a0-46c1-8232-8ad24ef8c2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
