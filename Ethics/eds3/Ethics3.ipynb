{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c1a546-959c-4b58-96ae-96e09ed44541",
   "metadata": {},
   "source": [
    "# Ethics 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0972a95-9b7f-49ec-8aa5-f4222d7d3a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.62"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "108*82/100+68*25/100+593*0.34 + 375*0.35 + 393 *0.24 + 0.07*341"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa34753-e264-4629-adb1-7d931cf9f075",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bias and Fairness\n",
    "\n",
    "Statistical correlation is symmetric, and this is way ...\n",
    "\n",
    "The concept of cause mathematically it has not been well defined until a few years ago. Cause is relating meanings with meanings (not numbers with numbers). \n",
    "\n",
    "## Association vs Causality:\n",
    "\n",
    "Association (or prediction) is using data to map some features of the world (the inputs) to other features of the world (the outputs). For example, 𝔼(Y | X, Z ).\n",
    "- We can estimate the expectation described, however, the value we seek for is not related or has no need of understanding the meaning of the features. \n",
    "\n",
    "> Q2: Estimate the mean income Y that would have been observed if all individuals had X = x1 (race=1) vs. if they had X ≠ x1 (race=2,3,4,5).\n",
    "\n",
    "Causal Inference is using data to predict certain features of the world if the world had been different. We cannot get these data by passive observation of the world! The world was different!\n",
    "- Q2 cannot be answered from the numbers, because it relays on the meaning of the variables. In fact, it is called the *Causal effect of Race on Income*. \n",
    "- We cannot use ML to answer this question because ML is based on patterns from data, and we do not have the data for this specific question. In fact, we cannot have this data on any possible way, because we may need to change the sex of every individual and study the impact, which is not possible, only analytically. \n",
    "- Answers to causal questions cannot be derived exclusively from p(X, Y, Z ). Answering a causal question (yes, sometimes is possible!) typically requires a combination of data, analytics, and expert causal knowledge.\n",
    "\n",
    "Sometimes we may be able to answer these questions, given a dataset, and sometimes we may not be. \n",
    "\n",
    "Observe we cannot approximate the definition of the variable we are seeking by the conditional expectation. If we do so, we are not considering other inputs affecting the income, other than race/gender (i.e. the one we are conditioning on). \n",
    "- Note that the `do` operator means that: if a sample has the value $x_1$, it is okay, but if it is not then consider as if it was $x_1$. This is, we are changing the race/gender of the person on the real individual. \n",
    "\n",
    "Let's say we have i.i.d. data sampled from some joint p(X, Y, Z ). Say we are ultimately interested in how variable Y behaves given X. At a high level, one can ask this question in two ways\n",
    "- Observational: based on the distributions given the variables we observe\n",
    "- Interventional: based on considering changes (`do`) on the distributions we have, by setting the values of a variables to a fixed value $x$. \n",
    "\n",
    "### Different possible generative models that are compatible with the observed distributions\n",
    "\n",
    "There are several models that could generate the dataset given. Behind any dataset there is a data generating process. A priori, by just observing the the distribution, we cannot determine which of the three generative models is the correct one. \n",
    "\n",
    "By intervining on the distribution, we can observe different reactions or outputs, from the intervined distribution. \n",
    "\n",
    "> Given data it is impossible to predict what would be the effect of the intervention. (intervining the data generating process): An intervention can be understood as a modification of the generative model of the data, producing a different probability distribution p(do(X ), Y, Z )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010a931-0aa8-41e7-a172-a14e22f66ac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can translate an intervention by sometring we can compute on the generation of the dataset. The way to translate the *interventional distributions* (the `do`) into an *observational distribution* by considering the graphs of interventions, where each arrow means cause of inference. By considering the reasoning on the causal graphs, we can create a process of translation from the interventions to the observed distributions. \n",
    "\n",
    "- We have to be aware of considering all the real world causing relations (arrows of causality). \n",
    "- We are considering directed graphs, so if we have a loop they do not work. If we have a loop, we can unloop the loop and break it into steps (like in recurrent neural networks)\n",
    "\n",
    "### Identifiability problem\n",
    "\n",
    "An **essential matter in causal inference** is that of a **query’s identifiability**. Given a causal query (for example, p(Y | do(X = 3))) for a certain DAG, we say it is identifiable if we can derive an statistical estimand (only using observational terms) for this query using the rules of do-calculus.\n",
    "\n",
    "\n",
    "Causal graphs:\n",
    "- Dashed lines correspond to unobserved confounders, associations produced by unobserved variables. They are some relations we know is there, but that we cannot measure. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbc15c-e89c-4089-8116-18b568ff98b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Causal Inference and do-calculus\n",
    "\n",
    "There are two ways to measure the causal relationship between two variables, X and Y:\n",
    "1. The easiest way is an intervention in the real world: You randomly force X to have different values and you measure Y\n",
    "    - This is what we do in Randomized Clinical Trial (RCT) or in an A/B Test. This is not always feasible (because of practical, ethical or economical reasons). \n",
    "2. Causal inference: If the query is identifiable, do-calculus allows us to massage p(X, Y, Z ) until we can express p(Y | do(X )) in terms of various marginals, conditionals and expectations under p(X, Y, Z )\n",
    "    - It only depends on the causal graph (it does not depend on data). Once we have defined the formula, then it depends on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6395e4b-ee0b-4cf4-b8af-b1b932184ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Causal inference Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e179b1c8-80a2-49b2-ae33-756bfcae2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import causaleffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de91d92-b669-4f02-ad17-4a4c97107747",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting causaleffect\n",
      "  Downloading causaleffect-0.0.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: numpy>=1.15.1 in /Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages (from causaleffect) (1.23.3)\n",
      "Collecting python-igraph>=0.8.3\n",
      "  Downloading python-igraph-0.10.4.tar.gz (9.5 kB)\n",
      "Collecting igraph==0.10.4\n",
      "  Downloading igraph-0.10.4-cp39-abi3-macosx_10_9_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting texttable>=1.6.2\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: python-igraph\n",
      "  Building wheel for python-igraph (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-igraph: filename=python_igraph-0.10.4-py3-none-any.whl size=9076 sha256=4e942f8ab4e608c6e22b8c72ead261cb020cbc209219e6f9b5e1bc9cd5ff9919\n",
      "  Stored in directory: /Users/flaviaferrusmarimon/Library/Caches/pip/wheels/dc/07/ac/bff79052fd6222d1239b228cd24a47222f227c2350f9c4df01\n",
      "Successfully built python-igraph\n",
      "Installing collected packages: texttable, igraph, python-igraph, causaleffect\n",
      "Successfully installed causaleffect-0.0.2 igraph-0.10.4 python-igraph-0.10.4 texttable-1.6.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install causaleffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710d0ba1-86ea-456e-aea2-1c3dabb37b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = causaleffect.createGraph(['X<->Y', 'Z->Y', 'X->Z', 'W->', 'W->Z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f15833a-8ded-42da-ad54-b35aff434d59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no library called \"cairo-2\" was found\nno library called \"cairo\" was found\nno library called \"libcairo-2\" was found\ncannot load library 'libcairo.so.2': dlopen(libcairo.so.2, 0x0002): tried: '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages/../../libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/bin/../lib/libcairo.so.2' (no such file), 'libcairo.so.2' (no such file), '/usr/local/lib/libcairo.so.2' (no such file), '/usr/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/UB/FPDS/Ethics/libcairo.so.2' (no such file)\ncannot load library 'libcairo.2.dylib': dlopen(libcairo.2.dylib, 0x0002): tried: '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages/../../libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/bin/../lib/libcairo.2.dylib' (no such file), 'libcairo.2.dylib' (no such file), '/usr/local/lib/libcairo.2.dylib' (no such file), '/usr/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/UB/FPDS/Ethics/libcairo.2.dylib' (no such file)\ncannot load library 'libcairo-2.dll': dlopen(libcairo-2.dll, 0x0002): tried: '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages/../../libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/bin/../lib/libcairo-2.dll' (no such file), 'libcairo-2.dll' (no such file), '/usr/local/lib/libcairo-2.dll' (no such file), '/usr/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/UB/FPDS/Ethics/libcairo-2.dll' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/df/4599m83s2vj4j1_h__gx7kqw0000gn/T/ipykernel_10064/1738124986.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcausaleffect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/causaleffect/graph.py\u001b[0m in \u001b[0;36mplotGraph\u001b[0;34m(g, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvisual_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvisual_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/igraph/drawing/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(obj, target, bbox, *args, **kwds)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mcairo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_cairo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mplotly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_plotly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/igraph/drawing/cairo/utils.py\u001b[0m in \u001b[0;36mfind_cairo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/cairocffi/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m cairo = dlopen(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mffi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cairo-2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cairo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'libcairo-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     ('libcairo.so.2', 'libcairo.2.dylib', 'libcairo-2.dll'))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/cairocffi/__init__.py\u001b[0m in \u001b[0;36mdlopen\u001b[0;34m(ffi, library_names, filenames)\u001b[0m\n\u001b[1;32m     42\u001b[0m     error_message = '\\n'.join(  # pragma: no cover\n\u001b[1;32m     43\u001b[0m         str(exception) for exception in exceptions)\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: no library called \"cairo-2\" was found\nno library called \"cairo\" was found\nno library called \"libcairo-2\" was found\ncannot load library 'libcairo.so.2': dlopen(libcairo.so.2, 0x0002): tried: '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages/../../libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/anaconda3/bin/../lib/libcairo.so.2' (no such file), 'libcairo.so.2' (no such file), '/usr/local/lib/libcairo.so.2' (no such file), '/usr/lib/libcairo.so.2' (no such file), '/Users/flaviaferrusmarimon/UB/FPDS/Ethics/libcairo.so.2' (no such file)\ncannot load library 'libcairo.2.dylib': dlopen(libcairo.2.dylib, 0x0002): tried: '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages/../../libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/anaconda3/bin/../lib/libcairo.2.dylib' (no such file), 'libcairo.2.dylib' (no such file), '/usr/local/lib/libcairo.2.dylib' (no such file), '/usr/lib/libcairo.2.dylib' (no such file), '/Users/flaviaferrusmarimon/UB/FPDS/Ethics/libcairo.2.dylib' (no such file)\ncannot load library 'libcairo-2.dll': dlopen(libcairo-2.dll, 0x0002): tried: '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages/../../libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/anaconda3/bin/../lib/libcairo-2.dll' (no such file), 'libcairo-2.dll' (no such file), '/usr/local/lib/libcairo-2.dll' (no such file), '/usr/lib/libcairo-2.dll' (no such file), '/Users/flaviaferrusmarimon/UB/FPDS/Ethics/libcairo-2.dll' (no such file)"
     ]
    }
   ],
   "source": [
    "causaleffect.plotGraph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6089e5ad-0729-4d53-a5ab-9fc405c77112",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cairocffi\n",
      "  Downloading cairocffi-1.5.0.tar.gz (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages (from cairocffi) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/flaviaferrusmarimon/anaconda3/lib/python3.9/site-packages (from cffi>=1.1.0->cairocffi) (2.20)\n",
      "Building wheels for collected packages: cairocffi\n",
      "  Building wheel for cairocffi (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cairocffi: filename=cairocffi-1.5.0-py3-none-any.whl size=90524 sha256=663d8eacbe9347ae18e1f3711eb939e368b47a5477af8517458d12fbc3207b03\n",
      "  Stored in directory: /Users/flaviaferrusmarimon/Library/Caches/pip/wheels/34/c0/21/58ab88c0e5a36a32f10f5257afbd6e5fae805171da13691863\n",
      "Successfully built cairocffi\n",
      "Installing collected packages: cairocffi\n",
      "Successfully installed cairocffi-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cairocffi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348fcf8-73be-4b42-b5c6-fad1903ca764",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Causal Inference and SCM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af7f71-8e51-4fdd-b21d-e9a4fe9307bb",
   "metadata": {},
   "source": [
    "We have computed the causal effect of something on some other variable of the total dataset. \n",
    "\n",
    "The causal diagram can be seen as a representation of an underlying structural causal model (generative model). A USCM is a graph where each node has the distribution given by a function. Therefore, there is a function that for each node gives us the value of the causal effect. Here we are considering the variables that the given nodes depends on, and some noise $e_j$. Once we have these functions, we can intervine the graph, and alter the generating process. \n",
    "\n",
    "**Counterfactual**: is causal estimation at the level of one single sample. This is, we can intervine in one efature of a singular individual. \n",
    "\n",
    "Given a certain observational sample e = (x_e, y_e, z_e) and an intervention do(X = xq), a counterfactual is the result of an hypothetical experiment in the past, what would have happened to the value of variable Y had we intervened on X by assigning value $x_q$.\n",
    "\n",
    "Identifiable counterfactuals can be computed as a three-step process by using a SCM:\n",
    "1. Abduction: estimate the triplete that provides the errors of each feature (this is, the expectation of the errors given the provided variables)\n",
    "2. Intervention: once we have estimated the errors, we intervine the graph, i.e. we apply the desired internvention `do(X=x)`\n",
    "3. Prediction: result of intervention, i.e. the counterfactual (it is the correct counterfactual if the graph is correct): $p(Y*|X* = x_0,X = x_e,Y = y_e,Z = z_e):=$ probability of getting a new outcome given the new value of $X$ and the old values of $X, Y, Z$ (factual, what we do have on our dataset). \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719e066-729b-4f28-91dd-14cc6b2dd640",
   "metadata": {},
   "source": [
    "## Causal Discrimination Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150821f-2f60-4f18-89c0-aad2339be937",
   "metadata": {},
   "source": [
    "Causal graph for Compass Prediction: \n",
    "- Dashed line: in the dataset there is a correlation between Age and Race, and we do not know why. Intuitivelly, there could not be any arrow between these two variables. However, on the dataset, there is a hidden correlation, we cannot explain. \n",
    "\n",
    "Uci Adult:\n",
    "- Predicting salary of the people in the USA. \n",
    "- Dashed line: again between Gender and age. Usually, when we do not know, it tends to be structural distrimination. \n",
    "- Gender-> Salary: this is discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09bcd4",
   "metadata": {},
   "source": [
    "# Ethics 4 \n",
    "\n",
    "## 12/04/23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349db4e-75c4-4249-8b66-349a9fcb83dc",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175c353-092d-4197-be40-4a03160c9020",
   "metadata": {},
   "source": [
    "There are different reasons that drive the demand for interpretability and explanations:\n",
    "- **Human curiosity and learning**: Humans have a mental model of their environment that is updated when something unexpected happens. This update is performed by finding an explanation for the unexpected event.\n",
    "- The goal of science is to **gain knowledge**, but many problems are solved with big datasets and **black box machine learning models**. The model itself becomes the source of knowledge instead of the data. Interpretability makes it possible to extract this additional knowledge captured by the model.\n",
    "    - two types of black-box AI systems: \n",
    "        - functions that are too complicated for any human to comprehend, \n",
    "            - The first kind of black-box AI includes deep neural networks, the architecture used in deep learning algorithms. DNNs are composed of layers upon layers of interconnected variables that become tuned as the network is trained on numerous examples. As neural networks grow larger and larger, it becomes virtually impossible to trace how their millions (and sometimes, billions) of parameters combine to make decisions. Even when AI engineers have access to those parameters, they won’t be able to precisely deconstruct the decisions of the neural network.\n",
    "        - and functions that are proprietary.\n",
    "            - The second type of black-box AI, the proprietary algorithms, is a reference to companies who hide the details of their AI systems for various reasons, such as intellectual property or preventing bad actors from gaming the system\n",
    "-  Machine learning models take on real-world tasks that require safety measures and testing\n",
    "- By default, **machine learning models pick up biases from the training data**. This can turn your machine learning models into racists that discriminate against protected groups. Interpretability can be a useful ethical debugging tool.\n",
    "- The process of integrating machines and algorithms into our daily lives requires interpretability to increase social acceptance and trust.\n",
    "\n",
    "Explainability may mean different things to different people in different contexts: \n",
    "-  For a **developer** (**RELIABILITY, GLOBAL/LOCAL INTERPRETABILITY**), to understand how their system is working, aiming to debug or improve it: to see what is working well or badly, and get a sense for why. \n",
    "- For a **user** (**TRUST**), \n",
    "    - to provide a sense for what the system is doing and why, to enable prediction of what it might do in unforeseen circumstances and build a sense of trust in the technology (**GLOBAL INTERPRETABILITY**).\n",
    "    - to understand why one particular prediction or decision was reached, to allow a check that the system worked appropriately and to enable meaningful challenge (e.g. credit approval or criminal sentencing). (**LOCAL INTERPRETABILITY**)\n",
    "- For **society** broadly to understand and become comfortable with the strengths and limitations of the system, overcoming a reasonable fear of the unknown. (**TRUST, GLOBAL INTERPRETABILITY**)\n",
    "- To provide an **expert** (perhaps a regulator) the ability to audit a prediction or decision trail in detail, particularly if something goes wrong (e.g. a crash by an autonomous car). (**RELIABILITY, GLOBAL/LOCAL INTERPRETABILITY**)\n",
    "\n",
    "IMP:\n",
    "- GLOBAL: GENERAL UNDERSTANDING OF HOW AN OVERALL SYSTEM WORKS\n",
    "- LOCAL: AN EXPLANATION OF A PARTICULAR PREDICTION OR DECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f68a99-ad10-4863-a672-ce399adf7d85",
   "metadata": {},
   "source": [
    "### TRANSPARENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1e619-927d-4ee7-8c76-97fd305b4b30",
   "metadata": {},
   "source": [
    "**A system with explainability capabilities is said to be more transparent.**\n",
    "\n",
    "As we have seen, there are many types of transparency with different motivations. Each case may need a different way to measure it. Actors with misaligned interests can **abuse transparency as a manipulation channel**, or inappropriately use information gained.\n",
    "**More transparency can also lead to less efficiency.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30718db3-dced-4c75-8fba-09c54d8b63ca",
   "metadata": {},
   "source": [
    "### EXPLANATIONS: THE SOCIAL SCIENCE PERSPECTIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95b3ff-03a1-47db-98a7-c782446f4aab",
   "metadata": {},
   "source": [
    "A good explanation is:\n",
    "- **Contrastive**: Contrastive. Humans usually do not ask why a certain prediction was made, but **why this prediction was made instead of anotherprediction**. The solution for the automated creation of contrastive explanations might also involve finding prototypes or archetypes in the data.\n",
    "- **Selected**: People do not expect explanations that cover the actualand complete list of causes of an event. We are used to selecting **one or two causes** from a variety of possible causes as THE explanation. \n",
    "- **Focused on the abnormal**:People focus more on causes that had a small probability but nevertheless happened.\n",
    "- **Consistent** with prior beliefs of the one who receives the explanation. This is difficult to integrate into machine learning!\n",
    "- **General and probable**: A cause that can explain many events is very general and could be considered a good explanation. Generality can easily be measured by the **feature’s support,** **which is the number of instances to which the explanation applies divided by the total number of instances**.\n",
    "\n",
    "**A principle of explicability, then, is an ethical\n",
    "principle that should help bring us closer to acceptable\n",
    "uses of algorithms.**\n",
    "\n",
    "European General Data Protection Regulation (GDPR)\n",
    "includes an indirect ‘right to explanation’ when fully\n",
    "automated decisions significantly affect someone: \n",
    "- the **data controller** shall implement suitable measures to safeguard the **data subject's rights and freedoms** and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and to contest the decision”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4860b-6304-4f06-920e-38e4483e2342",
   "metadata": {},
   "source": [
    "## ML-EXPLANATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068ba7b-8866-4189-9784-cd0057d8a6c3",
   "metadata": {},
   "source": [
    "Apunts a mà. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00053683-c520-4cd3-b11a-b5e25b2e671b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### INTERPRETABLE MODELS\n",
    "\n",
    "Models that **explain themselves**, such as decision trees, logistic regression, etc. \n",
    "\n",
    "The easiest way to achieve interpretability is to use a subset of algorithms that create interpretable models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36768ac",
   "metadata": {},
   "source": [
    "## Counterfactual explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e2e07",
   "metadata": {},
   "source": [
    "**Counterfactual explanations deal with the\n",
    "question: how should the features change to\n",
    "obtain a different outcome?**\n",
    "\n",
    "A counterfactual explanation describes a causal situation in the form:\n",
    "\n",
    "Counterfactuals are human-friendly explanations, because they are contrastive to the current instance and because they are selective, meaning they usually focus on a small number of feature changes. \n",
    "\n",
    "Observe that **counterfactuals are not unique**. This is called the Rashomon effect: there are usually multiple different counterfactual explanations for every situation. There are multiple counterfactuals that can change an event. \n",
    "\n",
    "If we can change an event by modifying a counterfactual that changes one feature it may be more effective than using one that changes two features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dc994",
   "metadata": {},
   "source": [
    "Explanation process:\n",
    "1. Define an alternative change in the prediction of an instance\n",
    "2. We look for a counterfactual instance(in the sense that there is a minimal alternative set of input variables, a counterfactual that changes the minimum number of features) hat produce the defined outcome. In order to be a counterfactual, the new values should be as similar as possible to the original ones. Another important requirement is that a counterfactual instance should have feature values that are likely.\n",
    "3. Sometimes it is useful to generate multiple, diverse counterfactuals.\n",
    "4. We report a local explanation:\n",
    "    - A counterfactual explanation describes a causal situation in the form ...diapo 78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1813fdb",
   "metadata": {},
   "source": [
    "Disadvantages: \n",
    "- It does not take into account the criterion: ”produce counterfactuals with only a few feature changes and likely feature values”\n",
    "- The method does not handle categorical features with many different levels well.\n",
    "- Counterfactuals are a causal concept, but the method is not based on causal inference. It can use non-meaningful counterfactuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c767a4",
   "metadata": {},
   "source": [
    "### Explanation of NLP classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3c6a6",
   "metadata": {},
   "source": [
    "A counterfactual in the sense of text is a minor change on a string, sentence or text, that generates a close output to the input. \n",
    "\n",
    "Does it make sense to take into account the style of a text when considering a NLP problem? \n",
    "- Things that affect our model, features, are more different to determine when considering text problems. It is not easy, following this example, to change the style of a text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b681cf9",
   "metadata": {},
   "source": [
    "### Advanced Topics\n",
    "\n",
    "We stop here, we don't see Advanced Topycs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad628ca",
   "metadata": {},
   "source": [
    "#### Interpretable Deep Learning\n",
    "\n",
    "Interesting free book on the topics of Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e27ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
