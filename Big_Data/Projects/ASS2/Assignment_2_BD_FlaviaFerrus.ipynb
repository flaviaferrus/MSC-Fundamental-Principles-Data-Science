{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "#### First Name: Flàvia \n",
    "#### Last Name: Ferrús Marimón\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"Twitter Analysis\")\\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_twitter = spark.read.json(\"corona_tweet_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- reply_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: string (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: string (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: string (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### From the user nestec col select the following cols only id_str,followers_count,friends_count and created at \n",
    "# (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- reply_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- user_id_str: string (nullable = true)\n",
      " |-- user_followers_count: long (nullable = true)\n",
      " |-- user_friends_count: long (nullable = true)\n",
      " |-- user_created_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select only the required columns from the nested user struct column\n",
    "user_cols = [\"id_str\", \"followers_count\", \"friends_count\", \"created_at\"]\n",
    "user_sub_df = df_twitter.select([col(\"user.\"+c).alias(\"user_\"+c) for c in user_cols])\n",
    "\n",
    "# Join the original DataFrame with the extracted user sub-DataFrame\n",
    "df_twitter1 = df_twitter.join(user_sub_df, how=\"left\")\n",
    "\n",
    "# Drop the original nested user column\n",
    "df_twitter1 = df_twitter1.drop(\"user\")\n",
    "\n",
    "# Print the resulting schema\n",
    "df_twitter1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58905625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the total count of number of records in df_twitter(1 point)\n",
    "df_twitter1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the source lable from source col by droping the anchor tab and save it as another col named extracted_source\n",
    "# for example <a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a> => Twitter Web App\n",
    "# you can use \"<a [^>]+>([^<]+)\" as regular expresion and the group would be 1 for this regular expression.\n",
    "#(4 points)\n",
    "#from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "#df_twitter=df_twitter.<FILL_IN>\n",
    "#df_twitter.select(col('extracted_source'),col('source')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|extracted_source|              source|\n",
      "+----------------+--------------------+\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "| Twitter Web App|<a href=\"https://...|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Extract the source label from the source column using a regular expression\n",
    "df_twitter2 = df_twitter1.withColumn(\"extracted_source\", regexp_extract(col(\"source\"), '<a [^>]+>([^<]+)', 1))\n",
    "\n",
    "# Select the extracted_source and source columns and show the resulting DataFrame\n",
    "df_twitter2.select(col('extracted_source'), col('source')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame into RDD\n",
    "rdd_twitter=df_twitter2.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a temporay table in memory with name as twitter (1 point)\n",
    "df_twitter2.createOrReplaceTempView(\"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id_str\n"
     ]
    }
   ],
   "source": [
    "cols = df_twitter2.columns\n",
    "print(cols[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mon Apr 20 15:01:57 +0000 2020',\n",
       "  5545,\n",
       "  [],\n",
       "  '1252251164227362821',\n",
       "  None,\n",
       "  None,\n",
       "  'India',\n",
       "  3460,\n",
       "  5477,\n",
       "  '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       "  '93612837',\n",
       "  121,\n",
       "  759,\n",
       "  'Mon Nov 30 11:38:08 +0000 2009',\n",
       "  'Twitter Web App'),\n",
       " ('Mon Apr 20 15:01:57 +0000 2020',\n",
       "  5545,\n",
       "  [],\n",
       "  '1252251164227362821',\n",
       "  None,\n",
       "  None,\n",
       "  'India',\n",
       "  3460,\n",
       "  5477,\n",
       "  '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       "  '346443880',\n",
       "  208,\n",
       "  1196,\n",
       "  'Mon Aug 01 08:15:42 +0000 2011',\n",
       "  'Twitter Web App')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_twitter.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Data\n",
    "\n",
    "#### You will be writing code to find the answer to the questions listed below using Just RDD, Using spark SQL \n",
    "\n",
    "- Analyze using RDD \n",
    "- Analyze using Dataframe without temp table \n",
    "- Analyze using spark.sql with temp table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Get total number of unique users (1 point for each type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using RDD\n",
    "\n",
    "## We need to get the total number of rows with unique user_id_str\n",
    "\n",
    "import pyspark\n",
    "#sc = pyspark.SparkContext('local[*]')\n",
    "# Display the type of the Spark Context sc\n",
    "#type(sc)\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Big Data Assisgnment 2\") \\\n",
    "    .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'93612837'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find the index of the column where we have the user id\n",
    "rdd_twitter.first()[10]\n",
    "\n",
    "## This is the id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rdd_twitter.user_id_str.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#unique_users = rdd_twitter.map(lambda x: x[10]).distinct().count()\n",
    "#print(unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this code takes a lot of memory, since the function `distinct` is not the optimum way to compute the unique number of users from the dataset. \n",
    "\n",
    "Observe that we can get the unique number of users by applying an alternative `rdd` code in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('93612837', 1), ('346443880', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = rdd_twitter.map(lambda x: (x[10], 1))\n",
    "users.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('93612837', 15894), ('2781749028', 15894)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users2 = users.reduceByKey(lambda a, b : a+b)\n",
    "unique_users2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "uniqueUserCount = unique_users2.count()\n",
    "print ('Unique users: %d' % uniqueUserCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unique users: 14094`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame\n",
    "#unique_users_df = df_twitter2.select(\"user_id_str\").distinct().count()\n",
    "#print(\"Total number of unique users: \", unique_users_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this code may take longer, since it considers the `distinct()` function. We can follow an approach similar to the previous one with the `rdd` object by considering: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "users_df = df_twitter2.groupBy(\"user_id_str\")\n",
    "unique_users2_df = users_df.agg(count(\"*\").alias(\"count\"))\n",
    "uniqueUserCount_df = unique_users2_df.count()\n",
    "print('Unique users: %d' % uniqueUserCount_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using `spark.sql` and the temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14094\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "\n",
    "## We use the table we have created by running the command:\n",
    "#df_twitter2.createOrReplaceTempView(\"twitter\")\n",
    "\n",
    "# Count the total number of unique users\n",
    "uniqueUserCount_sql = spark.sql(\"SELECT COUNT(DISTINCT user_id_str) FROM twitter\")\n",
    "\n",
    "# Print the result\n",
    "print(uniqueUserCount_sql.first()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Get count of user who have more than 1 tweet in the data (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the number of tweets per user, we do not need to compute it again, we just need to count the number of users that actually have more than 1 tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hosts: 14094\n"
     ]
    }
   ],
   "source": [
    "unique_users2.take(2)\n",
    "users_more1 = unique_users2.filter(lambda x: x[1] > 1)\n",
    "more1_UserCount = users_more1.count()\n",
    "print ('Unique hosts: %d' % more1_UserCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous case, we do not need to compute the aggregation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hosts with more than one tweet: 14094\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Count the number of unique users with more than one tweet\n",
    "#uniqueUserCount_df = df_twitter2.groupBy(\"user_id\").agg(count(\"*\").alias(\"count\")).filter(\"count > 1\").count()\n",
    "users_more1_df = unique_users2_df.filter(\"count > 1\")\n",
    "more1_UserCount_df = users_more1_df.count()\n",
    "print('Unique hosts with more than one tweet: %d' % more1_UserCount_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hosts with more than one tweet: 14094\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique users with more than one tweet using SQL\n",
    "more1_UserCount_sql = spark.sql(\"SELECT COUNT(DISTINCT user_id_str) FROM twitter HAVING COUNT(*) > 1\")\n",
    "#more1_UserCount_sql = spark.sql(\"SELECT COUNT(DISTINCT user_id_str) FROM twitter GROUP BY user_id_str HAVING COUNT(*) > 1\")\n",
    "\n",
    "# Print the result\n",
    "print('Unique hosts with more than one tweet: %d' % more1_UserCount_sql.first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that here we have used the `HAVING` command from the `spark.sql` set of functions, instead of the `WHERE`. This is because the `WHERE` clause is used to filter rows before they are grouped by an aggregate function. It operates on individual rows, and its condition is evaluated before the grouping is performed. The `WHERE` clause can only reference columns in the table being queried, and cannot reference the results of aggregate functions.\n",
    "\n",
    "Meanwhile, the `HAVING` clause is used to filter groups after they are created by an aggregate function. It operates on groups of rows that have been created by a `GROUP BY` clause, like in this case, and its condition is evaluated after the grouping is performed. The `HAVING` clause can reference aggregate functions, such as `COUNT()`, and can be used to filter groups based on the results of those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.3 Get total number unique extracted_source (1 point each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to repeat the previous approach but taking into account the `extracted_source` columns this time, isntead of the `user_id_str`. In order to do so, notice that this column corresponds to the last column of the data set created `twitter_df2`. Therefore, we can modify the code and consider the alternative column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'favorite_count', 'hashtags', 'id', 'in_reply_to_status_id', 'in_reply_to_user_id_str', 'location', 'reply_count', 'retweet_count', 'source', 'user_id_str', 'user_followers_count', 'user_friends_count', 'user_created_at', 'extracted_source']\n",
      "user_id_str\n",
      "15\n",
      "extracted_source\n"
     ]
    }
   ],
   "source": [
    "cols = df_twitter2.columns\n",
    "print(cols)\n",
    "print(cols[10])\n",
    "print(len(cols))\n",
    "print(cols[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sources: 133\n"
     ]
    }
   ],
   "source": [
    "sources = rdd_twitter.map(lambda x: (x[14], 1))\n",
    "unique_sources = sources.reduceByKey(lambda a, b : a+b)\n",
    "unique_sourcesCount = unique_sources.count()\n",
    "print ('Unique sources: %d' % unique_sourcesCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sources: 133\n"
     ]
    }
   ],
   "source": [
    "sources_df = df_twitter2.groupBy(\"extracted_source\")\n",
    "unique_sources_df = sources_df.agg(count(\"*\").alias(\"count\"))\n",
    "unique_sourcesCount_df = unique_sources_df.count()\n",
    "print('Unique sources: %d' % unique_sourcesCount_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sources: 133\n"
     ]
    }
   ],
   "source": [
    "unique_sourcesCount_sql = spark.sql(\"SELECT COUNT(DISTINCT extracted_source) FROM twitter\")\n",
    "print('Unique sources: %d' % unique_sourcesCount_sql.first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.4 Get top 5 most used extracted_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can order the data once we have already aggregated the number of times each `extracted_source` appears, and then take the top 5 from the sorted list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sources: [('Twitter for Android', 99528228), ('Twitter for iPhone', 90564012), ('Twitter Web App', 45742932), ('Twitter for iPad', 6802632), ('Twitter Web Client', 2161584)]\n"
     ]
    }
   ],
   "source": [
    "top_sources = unique_sources.takeOrdered(5, lambda s: -1 * s[1])\n",
    "print('Top 5 sources: %s' % top_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sources: DataFrame[extracted_source: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "top_sources_df = unique_sources_df.orderBy(\"count\", ascending=False).limit(5)\n",
    "print('Top 5 sources: %s' % top_sources_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sources:\n",
      "Twitter for Android\n",
      "Twitter for iPhone\n",
      "Twitter Web App\n",
      "Twitter for iPad\n",
      "Twitter Web Client\n"
     ]
    }
   ],
   "source": [
    "top_sources_list_df = top_sources_df.collect()\n",
    "\n",
    "print(\"Top 5 sources:\")\n",
    "for row in top_sources_list_df:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sources: DataFrame[extracted_source: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "# Query to get the top 5 most used sources\n",
    "top_sources_sql = spark.sql(\"SELECT extracted_source, COUNT(*) AS count \"\n",
    "                            \"FROM twitter \"\n",
    "                            \"GROUP BY extracted_source \"\n",
    "                            \"ORDER BY count DESC \"\n",
    "                            \"LIMIT 5\")\n",
    "\n",
    "print('Top 5 sources: %s' % top_sources_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sources:\n",
      "Twitter for Android\n",
      "Twitter for iPhone\n",
      "Twitter Web App\n",
      "Twitter for iPad\n",
      "Twitter Web Client\n"
     ]
    }
   ],
   "source": [
    "top_sources_list = top_sources_sql.collect()\n",
    "\n",
    "print(\"Top 5 sources:\")\n",
    "for row in top_sources_list:\n",
    "    print(row[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.5 Get count of distinct hastags used ( 5 point each) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may consider now the column corresponding to the `hastags`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags\n"
     ]
    }
   ],
   "source": [
    "print(cols[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe however that this could be more complex, since the `hastags` column contains all the possible hastags that are used in the particular tweet. Therefore, we may need to consider the different elements of the list as the particular hastags we want to count. \n",
    "\n",
    "Therefore, if we try to compute a similar code to the previous ones we will get the corresponding error since lists are not hashable and the `reduceByKey` function cannot operate if the key is not hashable. Thus, in order to solve this problem the `flatMap` function is used, taking the different possible hastags within each list from the column as the key of our `unique_hastags` rdd object. This is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boxing', 1),\n",
       " ('kickboxing', 1),\n",
       " ('covid19', 1),\n",
       " ('corona', 1),\n",
       " ('SelfQuarantine', 1),\n",
       " ('boxing', 1),\n",
       " ('kickboxing', 1),\n",
       " ('covid19', 1),\n",
       " ('corona', 1),\n",
       " ('SelfQuarantine', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hastags = rdd_twitter.map(lambda x: (x[2], 1))\n",
    "hashtags = rdd_twitter.flatMap(lambda x: [(tag, 1) for tag in x[2]])\n",
    "hashtags.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hastags: 1215\n"
     ]
    }
   ],
   "source": [
    "unique_hashtags = hashtags.reduceByKey(lambda a, b : a+b)\n",
    "unique_hashtagsCount = unique_hashtags.count()\n",
    "print ('Unique hastags: %d' % unique_hashtagsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I follow a similar approach for the case of the dataframe, here the `explode()` function is used to flatten the list of hashtags into separate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hashtags: 1215\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "hashtags_df_1 = df_twitter2.select(explode(\"hashtags\").alias(\"hashtag\"))\n",
    "hashtags_df = hashtags_df_1.groupBy(\"hashtag\")\n",
    "unique_hashtags_df = hashtags_df.agg(count(\"*\").alias(\"count\"))\n",
    "unique_hashtagsCount_df = unique_hashtags_df.count()\n",
    "print('Unique hashtags: %d' % unique_hashtagsCount_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the sql code, we use the `LATERAL VIEW` clause to explode the hashtags array column and create a new row for each hashtag, and then we count the distinct hashtags using the `COUNT(DISTINCT)` function like in the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hashtags: 1215\n"
     ]
    }
   ],
   "source": [
    "unique_hashtagsCount_sql = spark.sql(\"SELECT COUNT(DISTINCT hashtag) FROM twitter LATERAL VIEW EXPLODE(hashtags) AS hashtag\")\n",
    "print('Unique hashtags: %d' % unique_hashtagsCount_sql.first()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.6 Get top 5 hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that we already have the corresponding objects with the count for every `hashtag` it is direct to get the top 5, by applying similar code to the one on section 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hashtags: [('طبق_القدرات_للثانويه_ياريس', 6119190), ('Corona', 5070186), ('OilPrice', 3989394), ('COVID19', 1986750), ('corona', 1954962)]\n"
     ]
    }
   ],
   "source": [
    "top_hashtags = unique_hashtags.takeOrdered(5, lambda s: -1 * s[1])\n",
    "print('Top 5 hashtags: %s' % top_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somethign happened with this count, arabic numbers considered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hashtags: DataFrame[hashtag: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "top_hashtags_df = unique_hashtags_df.orderBy(\"count\", ascending=False).limit(5)\n",
    "print('Top 5 hashtags: %s' % top_hashtags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hashtags:\n",
      "طبق_القدرات_للثانويه_ياريس\n",
      "Corona\n",
      "OilPrice\n",
      "COVID19\n",
      "corona\n"
     ]
    }
   ],
   "source": [
    "top_hashtags_list_df = top_hashtags_df.collect()\n",
    "\n",
    "print(\"Top 5 hashtags:\")\n",
    "for row in top_hashtags_list_df:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first most used hashtag appears to be in arabic, but the count is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_hashtagsCount_sql = spark.sql(\"SELECT DISTINCT hashtag \"\n",
    "                                     \"FROM twitter LATERAL VIEW EXPLODE(hashtags) AS hashtag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_hashtagsCount_sql = spark.sql(\"SELECT hashtag \"\n",
    "                                     \"FROM twitter LATERAL VIEW EXPLODE(hashtags) AS hashtag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       hashtag|\n",
      "+--------------+\n",
      "|        boxing|\n",
      "|    kickboxing|\n",
      "|       covid19|\n",
      "|        corona|\n",
      "|SelfQuarantine|\n",
      "|        boxing|\n",
      "|    kickboxing|\n",
      "|       covid19|\n",
      "|        corona|\n",
      "|SelfQuarantine|\n",
      "|        boxing|\n",
      "|    kickboxing|\n",
      "|       covid19|\n",
      "|        corona|\n",
      "|SelfQuarantine|\n",
      "|        boxing|\n",
      "|    kickboxing|\n",
      "|       covid19|\n",
      "|        corona|\n",
      "|SelfQuarantine|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_hashtagsCount_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 hashtags: \n",
      "+--------------------+-------+\n",
      "|             hashtag|  count|\n",
      "+--------------------+-------+\n",
      "|طبق_القدرات_للثان...|6119190|\n",
      "|              Corona|5070186|\n",
      "|            OilPrice|3989394|\n",
      "|             COVID19|1986750|\n",
      "|              corona|1954962|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_hashtagsCount_sql.createOrReplaceTempView(\"hashtags\")\n",
    "top_hashtags_sql = spark.sql(\"SELECT hashtag as hashtag, COUNT(*) as count \"\n",
    "                             \"FROM hashtags \"\n",
    "                             \"GROUP BY hashtag \"\n",
    "                             \"ORDER BY count DESC \"\n",
    "                             \"LIMIT 5 \")\n",
    "print('Top 5 hashtags: ')\n",
    "top_hashtags_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Get total number of tweets which are retweeted more than 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retweet_count\n",
      "id\n"
     ]
    }
   ],
   "source": [
    "print(cols[8])\n",
    "print(cols[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here we need to get the different tweets we have, because if we directly filter by the tweets that have `retweet_count` greater than 100 we may count the tweets several times, when it is actually the same. Therefore, we may need to group by the tweet id, and then filter by the retweet count. \n",
    "\n",
    "However, we may see whether the retweet count increasses when the tweet is retweeted, or whether it starts on 0 again. If it starts on 0 and the tweet is apparently counted as a new tweet we need to add the retweet count. If not, we just need to group by tweet Id, since they all should have the same retweet count. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check which case we are in front of, what we should do is consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1)),\n",
       " ('1252251164227362821', (5477, 1))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = rdd_twitter.map(lambda x: (x[3], (x[8], 1)))\n",
    "tweets.take(10)\n",
    "#unique_tweets = tweets.reduceByKey(lambda a, b : a[1]+b[1])\n",
    "#unique_tweets.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all have the same retweet count when considering the tweet ID, even when different users retweeted it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If they all have the same count when the tweet id is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1252251164227362821', 5477), ('1252251166504824833', 6374)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 100\n",
    "tweets = rdd_twitter.map(lambda x: (x[3], x[8]))\n",
    "tweets.take(2)\n",
    "#unique_tweets = tweets.groupByKey()\n",
    "unique_tweets = tweets.reduceByKey(lambda a, b : max(a,b))\n",
    "unique_tweets.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1252251164227362821', 5477), ('1252251166504824833', 6374)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweeted = unique_tweets.filter(lambda x : x[1] > thresh)\n",
    "retweeted.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets retweeted more than 100 times: 15706\n"
     ]
    }
   ],
   "source": [
    "number_retweeted = retweeted.count()\n",
    "print('Number of tweets retweeted more than 100 times: %d' % number_retweeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If somehow the retweet count does not take into account all the aggregated retweets, then we have to aggregate them: (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresh = 100\n",
    "#tweets = rdd_twitter.map(lambda x: (x[3], x[8]))\n",
    "#unique_tweets = tweets.reduceByKey(lambda a, b : a+b)\n",
    "\n",
    "#retweeted = unique_tweets.filter(lambda x : x[1] > thresh)\n",
    "#number_retweeted = retweeted.count()\n",
    "#print('Number of tweets retweeted more than 100 times: %d' % number_retweeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "\n",
    "tweets_df = df_twitter2.groupBy(\"id\")\n",
    "unique_tweets_df = tweets_df.agg(max(\"retweet_count\").alias(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets retweeted more than 100 times: 15706\n"
     ]
    }
   ],
   "source": [
    "retweeted_df = unique_tweets_df.filter(\"count > 100\")\n",
    "number_retweeted_df = retweeted_df.count()\n",
    "print('Number of tweets retweeted more than 100 times: %d' % number_retweeted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using spark.sql and the temporay table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hosts with more than one tweet: 15847\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique users with more than one tweet using SQL\n",
    "number_retweeted_sql = spark.sql(\"SELECT COUNT(DISTINCT id) FROM twitter HAVING MAX(retweet_count) > 100\")\n",
    "\n",
    "# Print the result\n",
    "print('Number of tweets retweeted more than 100 times: %d' % number_retweeted_sql.first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number does not correspond to the previous results, therefore a different approach is conducted so the aggregation of the tweets per `id` is implemented correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets retweeted more than 100 times: 15706\n"
     ]
    }
   ],
   "source": [
    "count_retweeted_sql = spark.sql(\"SELECT COUNT(*) FROM (SELECT id, MAX(retweet_count) as max_retweets FROM twitter GROUP BY id) WHERE max_retweets > 100\")\n",
    "print('Number of tweets retweeted more than 100 times: %d' % count_retweeted_sql.first()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Get top 3 most retweeted tweets per country (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n"
     ]
    }
   ],
   "source": [
    "print(cols[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to group the information by `id` and `location`, and once we have this grouping, we can consider the aggregation corresponding to his key taking the max number of retweets, and sort the data to get the top 3 per grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1252251164227362821', 'India'), 5477),\n",
       " (('1252251164227362821', 'India'), 5477)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = rdd_twitter.map(lambda x: ((x[3], x[6]), x[8]))\n",
    "tweets.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1252251164227362821', 'India'), 5477),\n",
       " (('1252251164256555009', 'UK'), 6513)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from operator import max\n",
    "unique_tweets_country = tweets.reduceByKey(lambda a, b : a if a>b else b)\n",
    "unique_tweets_country.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', ('1252251164227362821', 5477)),\n",
       " ('UK', ('1252251164256555009', 6513))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tweets_ = unique_tweets_country.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "unique_tweets_.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', <pyspark.resultiterable.ResultIterable at 0x7ff4d20f5f30>),\n",
       " ('China', <pyspark.resultiterable.ResultIterable at 0x7ff4d20f5e10>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_tweets = unique_tweets_.groupByKey()\n",
    "grouped_tweets.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', (5477, '1252251164227362821')),\n",
       " ('UK', (6513, '1252251164256555009'))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "unique_tweets_rdd = unique_tweets_country.map(lambda x: (x[0][1], (x[1],x[0][0])))\n",
    "unique_tweets_rdd.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', <pyspark.resultiterable.ResultIterable at 0x7ff4d3b87310>),\n",
       " ('China', <pyspark.resultiterable.ResultIterable at 0x7ff4d3b86680>)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_tweets_rdd = unique_tweets_rdd.groupByKey()\n",
    "grouped_tweets_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('India', [(9988, '1252332114948874240'), (9976, '1252252336921206787'), (9973, '1252254519116746754')]), ('China', [(9998, '1252335780707684352'), (9993, '1252253596516843520'), (9984, '1252255562525560832')]), ('Mexico', [(9998, '1252253843145912320'), (9994, '1252255209776189442'), (9971, '1252252016006422533')]), ('USA', [(9994, '1252331777806524416'), (9987, '1252254239805579264'), (9982, '1252335464750735362')]), ('Pakistan', [(9988, '1252334264248606720'), (9975, '1252251912084357121'), (9973, '1252252126694309888')]), ('UK', [(9991, '1252333018578145280'), (9989, '1252252091822870529'), (9985, '1252254043973603329')]), ('Spain', [(9992, '1252335445876367361'), (9981, '1252334839094599681'), (9969, '1252254696112300032')]), ('Canada', [(9997, '1252335430323888128'), (9992, '1252254877939531776'), (9987, '1252252082825986051')]), ('Germany', [(9999, '1252334028092399622'), (9997, '1252330902325248000'), (9990, '1252252295510855682')]), ('Chile', [(9988, '1252253612140490759'), (9984, '1252334891951427585'), (9978, '1252253710182481920')]), ('Italy', [(9994, '1252252106750377996'), (9984, '1252251206027816960'), (9971, '1252330500670337024')])]\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "n = 3\n",
    "top_per_country_rdd = grouped_tweets_rdd.map(lambda x: (x[0], nlargest(n, x[1])))\n",
    "print( top_per_country_rdd.collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: India, Top Tweet IDs: [(9988, '1252332114948874240'), (9976, '1252252336921206787'), (9973, '1252254519116746754')]\n",
      "Country: China, Top Tweet IDs: [(9998, '1252335780707684352'), (9993, '1252253596516843520'), (9984, '1252255562525560832')]\n",
      "Country: Mexico, Top Tweet IDs: [(9998, '1252253843145912320'), (9994, '1252255209776189442'), (9971, '1252252016006422533')]\n",
      "Country: USA, Top Tweet IDs: [(9994, '1252331777806524416'), (9987, '1252254239805579264'), (9982, '1252335464750735362')]\n",
      "Country: Pakistan, Top Tweet IDs: [(9988, '1252334264248606720'), (9975, '1252251912084357121'), (9973, '1252252126694309888')]\n",
      "Country: UK, Top Tweet IDs: [(9991, '1252333018578145280'), (9989, '1252252091822870529'), (9985, '1252254043973603329')]\n",
      "Country: Spain, Top Tweet IDs: [(9992, '1252335445876367361'), (9981, '1252334839094599681'), (9969, '1252254696112300032')]\n",
      "Country: Canada, Top Tweet IDs: [(9997, '1252335430323888128'), (9992, '1252254877939531776'), (9987, '1252252082825986051')]\n",
      "Country: Germany, Top Tweet IDs: [(9999, '1252334028092399622'), (9997, '1252330902325248000'), (9990, '1252252295510855682')]\n",
      "Country: Chile, Top Tweet IDs: [(9988, '1252253612140490759'), (9984, '1252334891951427585'), (9978, '1252253710182481920')]\n",
      "Country: Italy, Top Tweet IDs: [(9994, '1252252106750377996'), (9984, '1252251206027816960'), (9971, '1252330500670337024')]\n"
     ]
    }
   ],
   "source": [
    "for country, tweet_ in top_per_country_rdd.collect():\n",
    "    print(\"Country: {0}, Top Tweet IDs: {1}\".format(country,tweet_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------+\n",
      "|location|                 id|max_retweets|\n",
      "+--------+-------------------+------------+\n",
      "|   China|1252251437469499393|         326|\n",
      "|     USA|1252251573046173697|        3424|\n",
      "|     USA|1252251719381237770|        7280|\n",
      "|Pakistan|1252251741350961153|         857|\n",
      "|   Chile|1252251780668325888|        4686|\n",
      "|Pakistan|1252252009891106816|        8128|\n",
      "|  Canada|1252252183787012098|        5912|\n",
      "|Pakistan|1252252354813935616|        1968|\n",
      "|  Mexico|1252252483516334080|        4640|\n",
      "| Germany|1252252700936417280|        8919|\n",
      "|Pakistan|1252252742346616834|        4616|\n",
      "|  Canada|1252252862073262080|        6700|\n",
      "|   Italy|1252252869379702788|        8316|\n",
      "|  Mexico|1252252931371433985|        5645|\n",
      "|  Mexico|1252252944977801217|        9269|\n",
      "|  Mexico|1252252988468420608|        2648|\n",
      "|   Spain|1252253024170450949|        4790|\n",
      "|     USA|1252253058970652672|        8746|\n",
      "|   Chile|1252253183604219904|        7569|\n",
      "|   India|1252253380153683969|         136|\n",
      "+--------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "tweets_country_df = df_twitter2.groupBy([\"location\", \"id\"]).agg({\"retweet_count\": \"max\"}) \\\n",
    "                    .withColumnRenamed(\"max(retweet_count)\", \"max_retweets\")\n",
    "tweets_country_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------+----+\n",
      "|location|                 id|max_retweets|rank|\n",
      "+--------+-------------------+------------+----+\n",
      "|  Canada|1252335430323888128|        9997|   1|\n",
      "|  Canada|1252254877939531776|        9992|   2|\n",
      "|  Canada|1252252082825986051|        9987|   3|\n",
      "|  Canada|1252336589809897473|        9977|   4|\n",
      "|  Canada|1252251718617722881|        9969|   5|\n",
      "|  Canada|1252331563351769094|        9966|   6|\n",
      "|  Canada|1252335441950408704|        9959|   7|\n",
      "|  Canada|1252254353764696064|        9947|   8|\n",
      "|  Canada|1252251536346042369|        9945|   9|\n",
      "|  Canada|1252330715066372102|        9929|  10|\n",
      "|  Canada|1252331297302855686|        9928|  11|\n",
      "|  Canada|1252251705783324672|        9926|  12|\n",
      "|  Canada|1252252615108329473|        9925|  13|\n",
      "|  Canada|1252251405668286466|        9920|  14|\n",
      "|  Canada|1252254639434719239|        9915|  15|\n",
      "|  Canada|1252252550327201792|        9908|  16|\n",
      "|  Canada|1252255993184092161|        9893|  17|\n",
      "|  Canada|1252331978214592512|        9890|  18|\n",
      "|  Canada|1252254174538010624|        9880|  19|\n",
      "|  Canada|1252332618688761862|        9866|  20|\n",
      "+--------+-------------------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a window function to rank the tweets by the max retweet count within each country\n",
    "window = Window.partitionBy(\"location\").orderBy(desc(\"max_retweets\"))\n",
    "ranked_tweets_df = tweets_country_df.withColumn(\"rank\", rank().over(window))\n",
    "ranked_tweets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------+\n",
      "|location|                 id|max_retweets|\n",
      "+--------+-------------------+------------+\n",
      "|  Canada|1252335430323888128|        9997|\n",
      "|  Canada|1252254877939531776|        9992|\n",
      "|  Canada|1252252082825986051|        9987|\n",
      "|   Chile|1252253612140490759|        9988|\n",
      "|   Chile|1252334891951427585|        9984|\n",
      "|   Chile|1252253710182481920|        9978|\n",
      "|   China|1252335780707684352|        9998|\n",
      "|   China|1252253596516843520|        9993|\n",
      "|   China|1252255562525560832|        9984|\n",
      "| Germany|1252334028092399622|        9999|\n",
      "| Germany|1252330902325248000|        9997|\n",
      "| Germany|1252252295510855682|        9990|\n",
      "|   India|1252332114948874240|        9988|\n",
      "|   India|1252252336921206787|        9976|\n",
      "|   India|1252254519116746754|        9973|\n",
      "|   Italy|1252252106750377996|        9994|\n",
      "|   Italy|1252251206027816960|        9984|\n",
      "|   Italy|1252330500670337024|        9971|\n",
      "|  Mexico|1252253843145912320|        9998|\n",
      "|  Mexico|1252255209776189442|        9994|\n",
      "+--------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for the top 3 most retweeted tweets per country\n",
    "top_tweets_country_df = ranked_tweets_df.filter(\"rank <= 3\") \\\n",
    "    .select(\"location\", \"id\", \"max_retweets\") \\\n",
    "    .orderBy(\"location\", desc(\"max_retweets\"))\n",
    "\n",
    "# Show the results\n",
    "top_tweets_country_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+\n",
      "|location|                 id|retweet_count|\n",
      "+--------+-------------------+-------------+\n",
      "|   Chile|1252253612140490759|         9988|\n",
      "|   Chile|1252253612140490759|         9988|\n",
      "|   Chile|1252253612140490759|         9988|\n",
      "|   China|1252335780707684352|         9998|\n",
      "|   China|1252335780707684352|         9998|\n",
      "|   China|1252335780707684352|         9998|\n",
      "| Germany|1252334028092399622|         9999|\n",
      "| Germany|1252334028092399622|         9999|\n",
      "| Germany|1252334028092399622|         9999|\n",
      "|   India|1252332114948874240|         9988|\n",
      "|   India|1252332114948874240|         9988|\n",
      "|   India|1252332114948874240|         9988|\n",
      "|   Italy|1252252106750377996|         9994|\n",
      "|   Italy|1252252106750377996|         9994|\n",
      "|   Italy|1252252106750377996|         9994|\n",
      "|  Canada|1252335430323888128|         9997|\n",
      "|  Canada|1252335430323888128|         9997|\n",
      "|  Canada|1252335430323888128|         9997|\n",
      "|  Mexico|1252253843145912320|         9998|\n",
      "|  Mexico|1252253843145912320|         9998|\n",
      "+--------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get top 3 most retweeted tweets per country\n",
    "top_tweets = spark.sql(\"SELECT location, id, retweet_count FROM \\\n",
    "                        (SELECT location, id, retweet_count, \\\n",
    "                         ROW_NUMBER() OVER (PARTITION BY location ORDER BY retweet_count DESC) as rank \\\n",
    "                         FROM twitter) \\\n",
    "                        WHERE rank <= 3\")\n",
    "\n",
    "# Show the result\n",
    "top_tweets.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 Total number of tweets per country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RDD (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find the unique tweets per country, and then count them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('India', '1252251164227362821'), 15894),\n",
       " (('UK', '1252251164256555009'), 15894)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "tweets = rdd_twitter.map(lambda x: ((x[6], x[3]), 1))\n",
    "unique_tweets_country = tweets.reduceByKey(add)\n",
    "unique_tweets_country.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', 1480), ('China', 1457)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Equivalent to\n",
    "## total_tweets = tweets.reduceByKey(lambda a,b : a+b)\n",
    "## checking if it is faster\n",
    "\n",
    "tweets_per_country = unique_tweets_country.map(lambda x: (x[0][0], 1)) # (x[1], x[0][1], 1)))\n",
    "total_tweets_country = tweets_per_country.reduceByKey(add)\n",
    "total_tweets_country.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: India, Tweet Count: 1480\n",
      "Country: China, Tweet Count: 1457\n",
      "Country: Mexico, Tweet Count: 1408\n",
      "Country: USA, Tweet Count: 1538\n",
      "Country: Pakistan, Tweet Count: 1470\n",
      "Country: UK, Tweet Count: 1375\n",
      "Country: Spain, Tweet Count: 1463\n",
      "Country: Canada, Tweet Count: 1441\n",
      "Country: Germany, Tweet Count: 1426\n",
      "Country: Chile, Tweet Count: 1409\n",
      "Country: Italy, Tweet Count: 1422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for country, tweet_count in total_tweets_country.collect():\n",
    "    print(\"Country: {0}, Tweet Count: {1}\".format(country,tweet_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using DataFrame (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|location|tweet_count|\n",
      "+--------+-----------+\n",
      "| Germany|       1426|\n",
      "|   China|       1457|\n",
      "|   India|       1480|\n",
      "|   Chile|       1409|\n",
      "|   Italy|       1422|\n",
      "|   Spain|       1463|\n",
      "|     USA|       1538|\n",
      "|  Mexico|       1408|\n",
      "|      UK|       1375|\n",
      "|  Canada|       1441|\n",
      "|Pakistan|       1470|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out duplicate tweets per location\n",
    "unique_tweets_per_country = df_twitter2.dropDuplicates([\"location\", \"id\"])\n",
    "\n",
    "# Count the number of unique tweets per country\n",
    "tweets_per_country = unique_tweets_per_country.groupBy(\"location\") \\\n",
    "    .agg({\"id\": \"count\"}) \\\n",
    "    .withColumnRenamed(\"count(id)\", \"tweet_count\")\n",
    "\n",
    "# Show the results\n",
    "tweets_per_country.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spark.sql and the temporay table. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|location|count|\n",
      "+--------+-----+\n",
      "| Germany| 1426|\n",
      "|   China| 1457|\n",
      "|   India| 1480|\n",
      "|   Chile| 1409|\n",
      "|   Italy| 1422|\n",
      "|   Spain| 1463|\n",
      "|     USA| 1538|\n",
      "|  Mexico| 1408|\n",
      "|      UK| 1375|\n",
      "|  Canada| 1441|\n",
      "|Pakistan| 1470|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct tweets per country using Spark SQL\n",
    "distinct_tweets_per_country = spark.sql(\"SELECT DISTINCT location, id \\\n",
    "                                         FROM twitter\")\n",
    "\n",
    "tweets_per_country = distinct_tweets_per_country.groupBy(\"location\") \\\n",
    "                                                .count() \\\n",
    "                                                .withColumnRenamed(\"country\", \"tweet_count\")\n",
    "\n",
    "# Show the result\n",
    "tweets_per_country.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 save the data such that you have seperate folder per country (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_at', 'string'),\n",
       " ('favorite_count', 'bigint'),\n",
       " ('hashtags', 'array<string>'),\n",
       " ('id', 'string'),\n",
       " ('in_reply_to_status_id', 'string'),\n",
       " ('in_reply_to_user_id_str', 'string'),\n",
       " ('location', 'string'),\n",
       " ('reply_count', 'bigint'),\n",
       " ('retweet_count', 'bigint'),\n",
       " ('source', 'string'),\n",
       " ('user_id_str', 'string'),\n",
       " ('user_followers_count', 'bigint'),\n",
       " ('user_friends_count', 'bigint'),\n",
       " ('user_created_at', 'string'),\n",
       " ('extracted_source', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the column `hashtags` is of type `array<string>` which cannot be converted correctly to the dataframe we want to store in the csv. Therefore, we may need to convert it to a string frist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_at', 'string'),\n",
       " ('favorite_count', 'bigint'),\n",
       " ('hashtags', 'array<string>'),\n",
       " ('id', 'string'),\n",
       " ('in_reply_to_status_id', 'string'),\n",
       " ('in_reply_to_user_id_str', 'string'),\n",
       " ('location', 'string'),\n",
       " ('reply_count', 'bigint'),\n",
       " ('retweet_count', 'bigint'),\n",
       " ('source', 'string'),\n",
       " ('user_id_str', 'string'),\n",
       " ('user_followers_count', 'bigint'),\n",
       " ('user_friends_count', 'bigint'),\n",
       " ('user_created_at', 'string'),\n",
       " ('extracted_source', 'string'),\n",
       " ('string_hashtags', 'string')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Replace 'array_column' with the actual name of your array column\n",
    "df_twitter3 = df_twitter2.withColumn('string_hashtags', concat_ws(',', 'hashtags'))\n",
    "\n",
    "df_twitter3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_at', 'string'),\n",
       " ('favorite_count', 'bigint'),\n",
       " ('id', 'string'),\n",
       " ('in_reply_to_status_id', 'string'),\n",
       " ('in_reply_to_user_id_str', 'string'),\n",
       " ('location', 'string'),\n",
       " ('reply_count', 'bigint'),\n",
       " ('retweet_count', 'bigint'),\n",
       " ('source', 'string'),\n",
       " ('user_id_str', 'string'),\n",
       " ('user_followers_count', 'bigint'),\n",
       " ('user_friends_count', 'bigint'),\n",
       " ('user_created_at', 'string'),\n",
       " ('extracted_source', 'string'),\n",
       " ('string_hashtags', 'string')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter4 = df_twitter3.drop('hashtags')\n",
    "df_twitter4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame\n",
    "# Write to CSV partitioned by 'location'\n",
    "df_twitter4.coalesce(1).write.partitionBy('location').format('csv').option('header', 'true').mode('overwrite').save('/stackOut/')\n",
    "\n",
    "#df_twitter2.write.partitionBy(\"location\").csv(\"tweets_per_country\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code keeps giving me an error that I think is related with the memory space. But in case of enough space this code should perform correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Save the data as parquet files (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataFrame\n",
    "\n",
    "df_twitter2.write.partitionBy(\"location\").mode(\"overwrite\").parquet(\"tweets_by_country.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
