{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aec7fe8-d788-41b0-8c38-5ebb53f61dda",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6ae79-ced0-43b7-9151-88873a3c6c9a",
   "metadata": {},
   "source": [
    "# Theory 7: Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697c96e-9eb2-4c18-9808-3cf6cd3d0ee6",
   "metadata": {},
   "source": [
    "- Hadoop is a platform that provides both distributed storage and computational capabilities.\n",
    "- Is was built as a batch processing: \n",
    "    - Batch processing: we store the data and isntead of processing it everytime, we process it once we have gathered it, every certain time. In batches. \n",
    "- It consist of one cycle of loading data from HDFS applying Map job then running the reduce job and finally writing the output back into HDFS.\n",
    "- In case of iterative computation the data needs to be loaded again!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed14cb-5319-491c-a964-ac7cf131d9d1",
   "metadata": {},
   "source": [
    "## Hadoop EcoSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55931a4-300d-45ea-9b13-bf4670afd3b4",
   "metadata": {},
   "source": [
    "To build a Hadoop Ecosystem, the necessary frameworks are the HDFS and YARN. All the other ones are useful and optional, depending on our applications on different aspects:\n",
    "- Hfive to run SQL Query, R Connectors for Statistics, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad914759-b07b-4b3e-b4a0-14023ef2e280",
   "metadata": {},
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a57d7-3438-40db-9765-6dc105e26da2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Kind of storage used in Hadoop.\n",
    "\n",
    "Advantages: Data Locality:\n",
    "- We can run it locally if our cluster is local, but generally, we run the clusters not in local. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea74316-7d77-4ce0-aef8-72bb1cd2ea61",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f4f32-6934-451f-b0cc-c3d0b1623475",
   "metadata": {},
   "source": [
    "####Â Replication management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec392b7-c69b-4aef-a8b7-848449fa1387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b81a7b84-dd23-4e6d-8496-5ecb9bf870df",
   "metadata": {},
   "source": [
    "- Block 1, for example exists in file 1,2 and 4. Thus we are creating blocks and storing them in different Nodes (DataNode). We have 1 file, for example the block 1, but we have 3 block nodes, because we are storing it in a different data node (shell). Thus, we are replicating each NameNode three times. If, for example we erase Block1 in one DataNode, we still have the remaining ones, because we have the copies in the other servers replicated. \n",
    "- If we destroy one Node, then we automatically pick one new node, or we can repare it and put it back in the cluster. If we destroy it, we lose the data, so we do not know how to find it anymore. Unless new NameNodes are replacing a Node that died, we lost the information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecf67a-797e-443c-837d-1d8a2952e1dd",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c05df3-8804-4cc4-9cd3-9862daa9f0c2",
   "metadata": {},
   "source": [
    "Kind of server storage in Amazon: it is a simple object storage service. \n",
    "- S3 is secure, there's encryption to the data that we store. (client or server side encryption). \n",
    "- It regularly verifies ...\n",
    "\n",
    "Object storage: (object-based storage) is in general a term that refers to the way in which we organize and work with units of storage, called objects. Every object contains 3 things:\n",
    "\n",
    "Block storage: files are split into evenly sized blocks of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b6ec0-ebb6-4b3a-9f46-fba349519479",
   "metadata": {},
   "source": [
    "#### Storage Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f60b9c-bb07-4b09-a21f-2e587c4063bb",
   "metadata": {},
   "source": [
    "We can have 2 types of data:\n",
    "- Accessed frequently \n",
    "- Accessed eventually\n",
    "\n",
    "S3 types:\n",
    "- S3 Standard for general-purpose storage of frequency accessed data. \n",
    "- S3 Intelligent-Tiering for data with unknown changing patterns\n",
    "- S3 Glacier: long term information (it is the cheapest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc468e7-e4e0-45ff-a731-7eae9aab8a7b",
   "metadata": {},
   "source": [
    "## Hadoop Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c7512-5953-430b-9689-b66eff718c95",
   "metadata": {},
   "source": [
    "Python is a Structure oriented language. It can also work with classes, using object oriented programming. There also are function oriented languages. \n",
    "\n",
    "Imagine we want to wirte a pyhton program to count words in a document: \n",
    "- We would split and count words\n",
    "    - Use a Hash Table: Everytime we see a new word we add a new iteration to the value of the corresponding key\n",
    "- However, if the document is too big, we follow the subsequent approach:\n",
    "    - We would split the document in different machines, follow the aforementioned process and the concat the Hash Table. What if the hashed table is too big?\n",
    "        - We can use Map Reduce: Once we split the document in different machines, we join the Hash tables, by aggregating the keys, in order in different machines. We have:\n",
    "            - a Map function (from big document to 4 machines). Takes as input a sentence and provides the ordanized dictionary describing the Hash Table. Map Stage. \n",
    "            - the Map Reduction (4 machines to reorganized and aggregated 4 machines). Takes as input the dictionaries, and gives a reorganized machine. Reduced Stage. \n",
    "        \n",
    "Here therefore, we are using functions, in terms of map functions and reduced functions, in order to create a complete program. \n",
    "\n",
    "Different phases in MapReduce:\n",
    "- Input\n",
    "- Input splits \n",
    "- Mapping \n",
    "- Shuffling: based on putting all the common keys together\n",
    "- Reducer\n",
    "- Final Output (which is a block of files)\n",
    "\n",
    "The opperation from Mapping to Shuffling is the most complex one, because we have to read from a different server and then shuffle it into different servers and rearrenge them. We have to write our map functions to create this shuffling. We have to be carefure, the less shuffle of data, the faster processing! We have to be smart when splitting, and creating balanced splittings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78384890-b9d3-4451-a666-fad0368e6d87",
   "metadata": {},
   "source": [
    "#### MapRecude Work Organization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55525c-991b-44bc-8c0c-9a69470b39fa",
   "metadata": {},
   "source": [
    "The complete execution process (execution of Map and Reduce tasks, boths) is controlled by 2 types of entities:\n",
    "- Jobtracker: acts like a master (responsible for complete execution of submitted job).\n",
    "- Multiple Task Trackers: Acts like slaves, each of them performing the job. \n",
    "\n",
    "We have the Job being submitted, then the Job Tracker on the NameNode coordinates the taskts among the Task Trackers, in different DataNodes. Each DataNode has a Task Tracker, managing the Map Task and the Reduce Task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ab203-a1d9-4a1f-9727-5410c9d4e7fb",
   "metadata": {},
   "source": [
    "### Hadoop 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56baa53f-b522-4baa-9453-61fb91ea1395",
   "metadata": {},
   "source": [
    "- Hadoop 1.0\n",
    "- Hadoop 2.0: also uses YARN:\n",
    "    - YARN is a resource manager. It enables the users to perform operations as per requirement by using a variety of tools. In this case, YARN manages the resources, in this case giving us the resources out of the clusters, to arrenge when to run it and how to organize it. \n",
    "    - YARN also performs Job Schedulling: it performs all our processing activities by allocating resources and shceduling tasks. \n",
    "    \n",
    "YARN Architecture: \n",
    "- Resource Manager: decides how much resources we give to whom. It is the ultimate authority in resource allocation\n",
    "    - Optimizes the cluster utilization like keeping all resources in use all the time against various constraints such as capacity guarantees, fairness, and SLAs. \n",
    "    - Two main components:\n",
    "        - Shceduler: allocating resources to the various running applications. If there is an application failure, the Schedulers does not guarantee to restart the failed tasks. \n",
    "        - Application Manager: accepting job submissions and negotiates the Resource Manager. \n",
    "- Node Manager: they run on the slave daemons and are responsible for the execution of a task on every single DataNode.\n",
    "    - It takes care of individual nodes in a Hadoop cluster and manages user jops and workflow on the given node. \n",
    "    - Goal: manage application containers assaingned to it by the resource manager. \n",
    "    - It also kills the container as directed by the Resource Manager. \n",
    "- Application Master: (applications understood as a task run, it starts and it closes. One application can have multiple jobs)\n",
    "    - Single job submitted to the framework. Each job application has a unique Application master associated with it which is a framework specific entity.\n",
    "    - It is the process that coordinates an application's execution in the cluster and also manages faults. When the job is done, the application master is killed. \n",
    "- Container: (not docker container!) are package of resourcing, including RAM, CPU, Network, HDD,... on a single node. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8711f-c33c-472a-a86f-197fcd109003",
   "metadata": {},
   "source": [
    "YARN Job Workflow:\n",
    "- Client -> RM\n",
    "- RM -> AM\n",
    "- AM -> RM\n",
    "- AM -> NM (the node manager gets from the client the desired resources) The node manager is the one that actually runs the task. The actual working is the Node Manager. All the other components are just necessary to coordinate all the work and the resources. \n",
    "- AM -> RM (the am tells the resource manager once the job is done, and dies). \n",
    "\n",
    "It is important to know this architecture, because once there are some failures it is important to know what actually failed and what happened. \n",
    "\n",
    "Kubernetes have very different architectures. \n",
    "\n",
    "Important videos to whatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131e7422-4362-458a-b71b-5918f5a4432b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(43200/60)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287eebf-5c08-4ab3-80e5-79462c4fd2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
